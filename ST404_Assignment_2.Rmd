---
title: "ST404_Assignment_2"
output:
  pdf_document: default
fontsize: 11pt
linestretch: 1.5
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r, include = FALSE}
#Loading the data
load('cancer.rdata')
attach(cancer)
#Storing raw data
cancerRaw <- cancer
#Loading packages
library(tidyr)
library(dplyr)
library(olsrr)
```
# Findings

# Statistical Methodology

Our findings are based off the preliminary EDA of the US Cancer Dataset.

## Cleaning The Data
### Missing Values
As declared in the EDA, missing values exist in PctEmployed16_Over and extremely low values exist in AvgHouseholdSize. We choose to remove the PctEmployed16_Over column in order to keep all 3047 counties within the dataset. 152 PctEmplyed16_Over values were missing which is a sizeable proportion in a dataset this size. Another option was to fill in the missing values by predicting them using a linear model created by the other variables. However, the variable PctUnemployed16_Over is very strongly negatively correlated with PctEmployed16_Over with a Pearson correlation coefficient of -0.65. Therefore, on balance, it is better to remove PctEmployed16_Over entirely to avoid multicollinearity with PctUnemployed16_Over, and to avoid increasing the variance of the model by performing further predictions of the missing values. We will clean the data in the following way:

* Scale the values between 0 and 0.1 in AvgHouseholdSize by 100.
* Remove the PctEmployed16_Over column containing missing values.

``` {r ScaleColumn, echo = FALSE}
# Multiply values less than 0.1 by 100 and replace column
cancer$AvgHouseholdSize[cancer$AvgHouseholdSize <= 0.1] <- cancer$AvgHouseholdSize[cancer$AvgHouseholdSize <= 0.1]*100
```
``` {r RemoveMissingValuesColumn, echo = FALSE}
cancer <- subset(cancer, select=-c(PctEmployed16_Over))
```


## Modelling Flow
Now that we have established the missing values and corrected these lets first look at the scatter plots of the data and fit a model based on observations of these. We looked at the correlation in our initial EDA and so we will use this to fit a model. The variables we select will have higher correlation with deathRate. The variables we will select are: Edu18_24, PctMarriedHouseholds, PctBlack, PctPublicCoverage, Pct PrivateCoverage, PctUnemployed16_Over, PercentMarried, PovertyPercent, medIncome and incidenceRate.

We will now assess the adequacy of the model. We want to know if the functional form of the model is correct. We will look at the following plots to assess this:
``` {r adequacy, echo = FALSE}
Model1 <- lm(deathRate ~ Edu18_24 + PctMarriedHouseholds + PctBlack + PctPublicCoverage + PctPrivateCoverage + PctUnemployed16_Over + PercentMarried + povertyPercent + medIncome + incidenceRate, data = cancer)
plot(Model1)
```

We can see from the plots that the variance in errors is not constant and we also notice from our intitial EDA that some individual predictor variables are not linear and show signs of heteroscedascity, so we will need to perform some transformations of some variables. 

We first transform the individual predictor variables to satisfy homoscedascity, these include the following transformations:

* log(PctBlack$+0.1$)
* log(incidenceRate)
* log(medianIncome)
* log(deathRate)

cancer5$incidenceRate=boxcox(cancer5$incidenceRate, p = 1.5) #using boxcox from practical class 2, user defined funciton in appendix
``` {r transformed variables, echo = FALSE}
cancer5=cancer

cancer5$incidenceRate=log(cancer5$incidenceRate)
cancer5$medIncome=log(cancer5$medIncome)
cancer5$PctBlack=log(cancer5$PctBlack+0.1)
cancer5$deathRate=log(cancer5$deathRate)

#Check for any updated heteroscedascity and linearity
plot(lm(deathRate ~ incidenceRate, data = cancer5))
plot(lm(deathRate ~ medIncome, data = cancer5))
plot(lm(deathRate ~ PctBlack, data = cancer5))

# Notice heteroscedacity has decreased for incidence rate, homoscedasticity exists for pctBlack and medIncome.
# Linearity for PctBlack, linearity for medIncome, incidenceRate is linear until a certain point
```
These transformations show evidence of homoscedascity and linearity thus we will update the new model.

``` {r adequacy2, echo = FALSE}
Model2 <- lm(deathRate ~ Edu18_24 + PctMarriedHouseholds + PctBlack + PctPublicCoverage + PctPrivateCoverage + PctUnemployed16_Over + PercentMarried + povertyPercent + medIncome + incidenceRate, data = cancer5)
plot(Model2)
```
We can see that the new model is linear and heteroscedascity has been reduced but it still exists. We will go on to calculate if any outliers exist and if they have high leverage.

Several predictor variables may have outliers that can be considered for removal if they have a high leverage, specifically, incidenceRate. We will look at Cook's distance to declare if any of these outliers have significant influence.

``` {r CheckOutliers, echo = FALSE}
library(car)
library(olsrr)
library(glmnet)

outlierTest(Model2)
# Shows some outliers exist, namely points 1093, 2713, 1941, 1220, 2645, 1058
par(mfrow=c(1,2))
ols_plot_resid_lev(Model2)
# This plot shows that the observations that are outliers with high leverage that we declare highly influencial are 2713, 1941 and 1059.
par(mfrow=c(1,2))
ols_plot_dfbetas(Model2)
```
Observations 1093, 2713, 1941, 1220, 2645 and 1058 are shown to be extreme outliers and so testing influence we see that 2713, 1941 and 1058 all have high leverage and furthermore DFBETAS greater than 0.3 in the model. We will declare this to be highly influencial and so remove these observations from our Model.

``` {r removeoutliers}
library(dplyr)
cancer5 <-  slice(cancer5, -c(2713, 1941, 1058))
```

Now we will refit the model. 

``` {r Model3}
Model3 <- lm(deathRate ~ Edu18_24 + PctMarriedHouseholds + PctBlack + PctPublicCoverage + PctPrivateCoverage + PctUnemployed16_Over + PercentMarried + povertyPercent + medIncome + incidenceRate, data = cancer5)
plot(Model3)
```

``` {r checklargesample, include = FALSE}
summary(cancer5)
#Shows 3044 observations 
```
We have 3044 observations in our sample space, so this is large. What does this mean?

We will now check to see if deathRate highly correlates to any of the explanatory variables by using analysis of variance.
``` {r anova}
Anova(Model3)
summary(Model3)
```
From the F statistic with a p-value <0.05 we can determine that our response variable has a relationship with at least one of the variables in the model and we can further test which model is the optimal model.

Let's check to see if there is any redundancy in the model. We can do this by checking for multicollinearity within the explanatory variables. 

``` {r redundancy}
# Look at scatter plots and compare weak positive correlations to the correlations shown in original plots.
par(mfrow = c(2,3))
with(cancer5, scatter.smooth(deathRate~Edu18_24, ylab = "deathRate", xlab = "Edu18_24", lpars = list(col = "red", lwd = 2), main = "deathRate against Edu18_24", cex.main = 1))
with(cancer5, scatter.smooth(deathRate~PctMarriedHouseholds, ylab = "deathRate", xlab = "PctMarriedHouseholds", lpars = list(col = "red", lwd = 2), main = "deathRate against PctMarriedHouseholds", cex.main = 1))
with(cancer5, scatter.smooth(deathRate~PctBlack, ylab = "deathRate", xlab = "PctBlack", lpars = list(col = "red", lwd = 2), main = "deathRate against PctBlack", cex.main = 1))
with(cancer5, scatter.smooth(deathRate~PctPrivateCoverage, ylab = "deathRate", xlab = "PctPrivateCoverage", lpars = list(col = "red", lwd = 2), main = "deathRate against PctPrivateCoverage", cex.main = 1))
with(cancer5, scatter.smooth(deathRate~PctPublicCoverage, ylab = "deathRate", xlab = "PctPublicCoverage", lpars = list(col = "red", lwd = 2), main = "deathRate against PctPublicCoverage", cex.main = 1))
with(cancer5, scatter.smooth(deathRate~povertyPercent, ylab = "deathRate", xlab = "povertyPercent", lpars = list(col = "red", lwd = 2), main = "deathRate against povertyPercent", cex.main = 1))
with(cancer5, scatter.smooth(deathRate~medIncome, ylab = "deathRate", xlab = "medIncome", lpars = list(col = "red", lwd = 2), main = "deathRate against medIncome", cex.main = 1))
with(cancer5, scatter.smooth(deathRate~incidenceRate, ylab = "deathRate", xlab = "incidenceRate", lpars = list(col = "red", lwd = 2), main = "deathRate against incidenceRate", cex.main = 1))
```
The scatter plots show a negative correlation between deathRate and PctMarriedHouseholds and medIncome, which are not negative in the summary of the model. Moreover, the estimate is negative for PovertyPercent in the summary statistics but clearly not in the plot. This shows there exists redundancy and can be explained potentially by any collinearity.

``` {r multicollinearity}
library(olsrr)
ols_coll_diag(Model3)
#Look for low tolerance, VIF > 5 and condition index > 30
```
This shows that PctMarriedHouseholds, PctPrivateCoverage, PercentMarried, povertyPercent and medIncome all had VIF >5. When looking at the condition index's above 30, the one variance decomposition proportion above 0.3 which is said to be the cut off is povertyPercent when the condition index is 547.2. We can also look at the correlation matrix to check which variables povertyPercent is highly correlated with, as seen in assignment one. This shows high correlation with all the variables with high VIF. So, we will perform variable selection to conclude a final model due to redundancy in the model.



## Transformations

Due to the large quantity of variables, it is important to keep the model simple so that it is easy to interpret and maintains explanatory power.

Firstly, looking at skewed variables, we could perform transformations of the left-skewed variables PctPrivateCoverage and PercentMarried, and the right-skewed variables PctBlack (after a shift of 0.1 due to some values being 0), IncidenceRate, medIncome, AvgHouseholdSize, PctUnemployed16_Over and PovertyPercent. This does not totally remove skew from PctBlack but does reduce it substantially. These transformations were discussed in assignment 1 and are listed below.

Secondly, to deal with variables showing signs of heteroscedasticity, we could log transform deathRate, the dependent variable. This includes the variables medIncome, PctPrivateCoverage, PctEmpPrivCoverage and PctPublicCoverage.

List of potential transformed variables with associated transformation:

* PctPrivateCoverage^2^
* PercentMarried^2^
* log(PctBlack$+0.1$)
* log(IncidenceRate)
* log(medianIncome)
* log(AvgHouseholdSize)
* PctUnemplyed16_over^$\frac{1}{2}$^
* PovertyPercent^$\frac{1}{5}$
* log(deathRate)

The following residual plots will allow us to see if homoscedasticity is fixed for these variables after transformations.

```{r, include=FALSE}
#defining boxcox function
boxcox <- function(x, p){ 
  if (p == 0){
    return(log(x)) 
  }
  else{
    return((x^p - 1)/p)
  } 
}
```

``` {r transformations, echo = FALSE}
cancer2=cancer
cancer2$deathRate=log(cancer2$deathRate)
cancer2$incidenceRate=boxcox(cancer2$incidenceRate, p = 1.5) #using boxcox from practical class 2, user defined funciton in appendix
cancer2$medIncome=log(cancer2$medIncome)
cancer2$AvgHouseholdSize=log(cancer2$AvgHouseholdSize)
cancer2$PercentMarried=cancer2$PercentMarried^2
cancer2$PctUnemployed16_Over=sqrt(cancer2$PctUnemployed16_Over)
cancer2$PctPrivateCoverage=cancer2$PctPrivateCoverage^2
cancer2$PctBlack=log(cancer2$PctBlack+0.1)
```

``` {r residualplots, echo = FALSE}
par(mfrow=c(1,4))
plot(lm(deathRate ~ incidenceRate, data=cancer2), 1)
plot(lm(deathRate ~ medIncome, data=cancer2), 1)
plot(lm(deathRate ~ AvgHouseholdSize, data=cancer2), 1)
plot(lm(deathRate ~ PercentMarried, data=cancer2), 1)
plot(lm(deathRate ~ PctUnemployed16_Over, data=cancer2), 1)
plot(lm(deathRate ~ PctPrivateCoverage, data=cancer2), 1)
plot(lm(deathRate ~ PctBlack, data=cancer2), 1)
```

We also add a variable describing the region in which the county is in. We split the US into 4 regions defined by the Census Bureau-designated regions and divisions. This division was chosen as it is the most common division for the purposes of data analysis since it groups states into groups with similar geographical features and cultures among other factors. These regions are Northeast, Midwest, South and West.

``` {r region variable, echo=FALSE}
cancer2 = separate(cancer2,"Geography",c("County","State"),sep = ", ")
cancer2 <- cancer2 %>%
  mutate(Region = case_when(
    State %in% c("New Hampshire","New Jersey","New York","Maine","Massachusetts","Vermont","Connecticut","Pennsylvania","Rhode Island") ~ "Northeast",
    State %in% c("Wisconsin","Nebraska","Michigan","Minnesota","North Dakota","Missouri","Kansas","Ohio","Indiana","Iowa","Illinois","South Dakota") ~ "Midwest",
    State %in% c("West Virginia","Virginia","North Carolina","Alabama","Arkansas","Tennessee","Texas","Louisiana","Maryland","Mississippi","Kentucky","Delaware","District of Columbia","Florida","Oklahoma","South Carolina","Georgia") ~ "South",
    State %in% c("Washington","Nevada","New Mexico","California","Montana","Utah","Colorado","Wyoming","Oregon","Hawaii","Idaho","Alaska","Arizona") ~ "West"
  ))
cancer2$Region <-  factor(cancer2$Region)
```

## Modelling and Variable Selection
### Stepwise Regression

**Initial Ideas**

Before attempting to fit any models or try stepwise regression, we looked at what variables we thought would be most important in determining the death rate within a county. We came to the conclusion that incidence rate would be the most important. Median income within a region as well as the type of health coverage in that region would be important. The median age in a county may also affect the death rate. Additionally, an intercept may need to be excluded as it makes no sense to predict the death rate to be greater than zero when all variables (particularly incidence rate) are equal to zero.

We also looked at combinations of variables that could be problematic if they are in the same model. This was based heavily on the correlation between variables, which is visualised by the matrix below: 

```{r, echo=FALSE,message=FALSE}
require(corrplot)
correlations1 <- cor(cancer[,c(2,3,5:17)],method='spearman')
corrplot(correlations1,method='square',title='Spearman Correlation plot', tl.col="black", tl.cex=0.6,mar=c(1,1,1,1))
```

The matrix shows that we should not include more than one variable explaining the type of health coverage, both of `povertyPercent` and `medIncome`, both of the median age variables and both of the marriage rate variables. 

**Without transformations**

Initially we attempted stepwise regression for the data set without any tranformations. Despite using both AIC and BIC, we had issues with there being too many parameters (10-12 parameters) and this meant that the model was not simple. As a result we chose to apply the transformations and see if that gave better results.

**With transformations**

First we apply the transformations and create a separate transformed dataset, then we define the bounds of the complexity of our model. Then we do the same as above and perform stepwise regression on this dataset using both AIC and BIC.

```{r Apply transformations, include=FALSE}
cancer2=cancer
cancer2$incidenceRate=incidenceRate^(3/2)
cancer2$medIncome=log(medIncome)
cancer2$PctBlack = log(PctBlack+0.1)
cancer2 = separate(cancer2,"Geography",c("County","State"),sep = ", ")
cancer2 <- cancer2 %>%
  mutate(Region = case_when(
    State %in% c("New Hampshire","New Jersey","New York","Maine","Massachusetts","Vermont","Connecticut","Pennsylvania","Rhode Island") ~ "Northeast",
    State %in% c("Wisconsin","Nebraska","Michigan","Minnesota","North Dakota","Missouri","Kansas","Ohio","Indiana","Iowa","Illinois","South Dakota") ~ "Midwest",
    State %in% c("West Virginia","Virginia","North Carolina","Alabama","Arkansas","Tennessee","Texas","Louisiana","Maryland","Mississippi","Kentucky","Delaware","District of Columbia","Florida","Oklahoma","South Carolina","Georgia") ~ "South",
    State %in% c("Washington","Nevada","New Mexico","California","Montana","Utah","Colorado","Wyoming","Oregon","Hawaii","Idaho","Alaska","Arizona") ~ "West"
  ))
cancer2$Region <-  factor(cancer2$Region)
```

We first attempted to use AIC but this returned a model with 11 variables in it as well as an intercept term. Despite the fact this model is a good explanation of the data, it is far from simple and would be difficult to interpret properly. As a result we opted to go forward using BIC as this gave a heavier penalty for having more variables.

```{r,include=FALSE}
fullTransModel=lm(deathRate~incidenceRate+medIncome+povertyPercent+MedianAgeMale+
                    MedianAgeFemale+AvgHouseholdSize+PercentMarried+
                    PctUnemployed16_Over+PctPrivateCoverage+PctEmpPrivCoverage+
                    PctPublicCoverage+PctBlack+PctMarriedHouseholds+Edu18_24+Region,data=cancer2)
nullTransModel=lm(deathRate~1,data=cancer2)
bicTransModel=step(nullTransModel,direction='both',scope=formula(fullTransModel),trace=0,k=log(3047))
summary(bicTransModel)
ols_coll_diag(bicTransModel)
extractAIC(bicTransModel,k=log(3047))
```

From this first model the most notable issues were still a lot of variables, 9 in total, which still is not very simple. Additionally the condition number was very high, 369, showing key issues between `medIncome` and the intercept term. `medIncome` was a variable we believed to be important for the model but, regardless of the approach we took to solve the issue, it always needed to be removed. Even though it was deemed to be important, there are other variables in the data which would represent similar ideas, for example `povertyPercent`. As a result we changed our maximal model to not include it and so going forward, `medIncome` was not used. We repeated our stepwise regression without `medIncome`. Once this is done, the diagnostic were are analysed, the Residuals vs Leverage plot is shown.

```{r,include=FALSE}
fullTransModel=lm(deathRate~incidenceRate+povertyPercent+MedianAgeMale+
                    MedianAgeFemale+AvgHouseholdSize+PercentMarried+
                    PctUnemployed16_Over+PctPrivateCoverage+PctEmpPrivCoverage+
                    PctPublicCoverage+PctBlack+PctMarriedHouseholds+Edu18_24+Region,data=cancer2)
nullTransModel=lm(deathRate~1,data=cancer2)

bicTransModel=step(nullTransModel,direction='both',scope=formula(fullTransModel),k=log(3047))
```

```{r,include=FALSE}
summary(bicTransModel)
plot(bicTransModel, which=4)
```

The diagnostic plots for this model showed a high influence point which may have needed to be removed, this was the point for Williamsburg City, Virginia. A point that had an unusually high incidence rate was Union County, Florida. A third of the population in this county live in prisons$^{[1]}$ which is not representative of the rest of the USA, as a result we also chose to remove this point. We then continued with stepwise regression. 

```{r,include=FALSE}
cancer2=cancer2[-which(cancer2$incidenceRate>30000),]
fullTransModel=lm(deathRate~incidenceRate+povertyPercent+MedianAgeMale+
                    MedianAgeFemale+AvgHouseholdSize+PercentMarried+
                    PctUnemployed16_Over+PctPrivateCoverage+PctEmpPrivCoverage+
                    PctPublicCoverage+PctBlack+PctMarriedHouseholds+Edu18_24+Region,data=cancer2)
nullTransModel=lm(deathRate~1,data=cancer2)
bicModelNoOut=step(nullTransModel,direction='both',scope=formula(fullTransModel),k=log(3045))
ols_coll_diag(bicModelNoOut)
extractAIC(bicModelNoOut,k=log(3045))
#BIC = 18263.02
#Condition number = 98.7

#Remove intercept and PercentMarriedHouseholds
bicModelNoOut2=lm(deathRate~0+incidenceRate+povertyPercent+
                    PctUnemployed16_Over+PctPrivateCoverage+PctEmpPrivCoverage+
                    PctPublicCoverage+PctBlack+Edu18_24+
                    Region,data=cancer2)
summary(bicModelNoOut2)
ols_coll_diag(bicModelNoOut2)
extractAIC(bicModelNoOut2,k=log(3045))
#BIC = 18268.4
#Condition number = 71.3

#Remove PctPrivateCoverage
bicModelNoOut3=lm(deathRate~0+incidenceRate+povertyPercent+
                    PctUnemployed16_Over+PctEmpPrivCoverage+
                    PctPublicCoverage+PctBlack+Edu18_24+
                    Region,data=cancer2)
summary(bicModelNoOut3)
ols_coll_diag(bicModelNoOut3)
extractAIC(bicModelNoOut3,k=log(3045))
#BIC = 18298.2
#Condition number = 59.8

#Remove Edu18_24
bicModelNoOut4=lm(deathRate~0+incidenceRate+povertyPercent+
                    PctUnemployed16_Over+PctEmpPrivCoverage+
                    PctPublicCoverage+PctBlack+
                    Region,data=cancer2)
summary(bicModelNoOut4)
ols_coll_diag(bicModelNoOut4)
extractAIC(bicModelNoOut4,k=log(3045))
#BIC = 18355.2
#Condition number = 46.8

#Remove PctEmpPrivCoverage
bicModelNoOut5=lm(deathRate~0+incidenceRate+povertyPercent+
                    PctUnemployed16_Over+PctPublicCoverage+PctBlack+
                    Region,data=cancer2)
summary(bicModelNoOut5)
ols_coll_diag(bicModelNoOut5)
extractAIC(bicModelNoOut5,k=log(3045))
#BIC = 18360.26
#Condition number = 23.1
plot(bicModelNoOut5)
```

The initial model included the following variables: `incidenceRate`, `PctPrivateCoverage`, `Region`, `PctPublicCoverage`, `PctEmpPrivCoverage`, `PctUnemployed16_Over`, `Edu18_24`, `povertyPercent`, `PctBlack` and `PctMarriedHousehlds`. There were significant issues with collinearity here and the number of variables should be reduced to help with the simplicity of the model so that it can be interpreted. A change log of how we got to our final version of this model is below. The aim was to find a balance such that the condition numbers were low enough and the BIC was not affected too much.

```{r Change log,echo=FALSE}
changes=matrix(0,nrow=5,ncol=4)
BICs=c(18263.02,18268.4,18298.2,18355.2,18360.26)
CondNos=c(98.7,71.3,59.8,46.8,23.1)
Excluded=c('NA','Intercept','PctPrivateCoverage','Edu18_24','PctEmpPrivCoverage')
changes[,1]=1:5
changes[,2]=Excluded
changes[,3]=BICs
changes[,4]=CondNos
changes=as.table(changes)
colnames(changes)=c('Model no.','Removed variable','BIC','Condition Number')
knitr::kable(changes, caption = "Change log for the model")
```

This gives a final model which is shown in the table below in detail.

```{r Model Table,echo=FALSE}
name=c('incidenceRate','povertyPercent','PctUnemployed16_Over',
       'PctPublicCoverage','PctBlack','Region(Midwest)','Region(Northeast)',
       'Region(South)','Region(West)')
transformation=c('incidenceRate^(3/2)','None','None','None','log(PctBlack+0.1)',
                'None','None','None','None')
pVals=c('<2e-16','<2e-16','3.1e-11','<2e-16','9.1e-6','<2e-16','<2e-16','<2e-16',
        '<2e-16')
beta=c('0.00698','0.803','1.02','0.592','-1.90','71.5','61.0','78.7','56.7')
modelTable=as.table(matrix(data=c(transformation,pVals,beta),nrow=9,ncol=3))
rownames(modelTable)=name
colnames(modelTable)=c('Transformation','p-value','Parameter estimate')
knitr::kable(modelTable, caption = "Information on the final model")
```

**Interpretation**

For incidence rate the value of the parameter estimate is 0.00696. Since the variable is raised to the power of 1.5, this means that if the incidence rate quadruples, then the death rate will increase by 8 times. For example, if the incidence rate changes from 100 to 400 cases per 100000 people, then the death rate will increase from 6.96 to 55.68, if all other variables are equal to 0. The other transformed variable is `PctBlack`, for this variable a parameter estimate of -1.03 means that if the percentage of black population increases then the death rate will decrease. The exact amount of the death rate decrease is dependent on what your percentage increases from and to but the marginal decrease in death rate also decreases as the percentage of black population increases. For example an observed increase in black population from 1% to 2% will result in a larger decrease in death rate than 50% to 51%.

For `povertyPercent` the parameter estimate is 0.768, this means that for every extra percent of the population of the county living in poverty, the cancer death rate increases by 0.768. For `PctUnemployed16_Over`, the parameter estimate is 0.983, this means that if the percentage of the unemployed population in a county increases by 1% then the cancer death rate increases by 0.983. Finally the regional parameters all correspond to an increase in death rate. If we take the west as the reference parameter then, in comparison, if the county is in the Midwest then the death rate will be 14.0 per 100000 people higher than the west. If it is in the South then the death rate will be 19.3 per 100000 people higher and if it is in the north east then the death rate will be 2.7 per 100000 people higher.

All of the variables also have low p-values, they are all significant at the 0.5% significance level which means all of their inclusions in the model are justified. Finally, the F-statistic for the model has a value of 26740 on 9 and 3036 degrees of freedom. This is a high value for the F-statistic and it has an extremely low p-value, less than 2.2e-16, this confirms that the model is a much better fit for the data than a model with just an intercept.

**Cross validation**

To determine how well the model predicts values we use leave one out cross validation (LOOCV). This is included in the package `caret`. First LOOCV was used on the model that the BIC stepwise regression function suggested and then it was used on the altered model where multicollinearity issues were fixed. These results were compared to see how well the final model predicts values in comparison to the model stepwise regression suggests. A summary of the output is given below.

```{r Cross validation, include=FALSE,eval=FALSE}
library(caret)
ctrl <- trainControl(method = "LOOCV")
model <- train(deathRate~0+incidenceRate+povertyPercent+
                 PctUnemployed16_Over+PctPublicCoverage+PctBlack+
                 Region,data=cancer2,
               method = "lm", trControl = ctrl)
print(model)
#RMSE = 20.21833
#MAE = 15.18691
#RSquared = 0.461552

BICmodel <- train(deathRate~0+incidenceRate+PctPrivateCoverage+Region+
                    PctPublicCoverage+PctEmpPrivCoverage+PctUnemployed16_Over+
                    Edu18_24+povertyPercent+PctBlack+PctMarriedHouseholds,
                  data=cancer2, method = "lm", trControl = ctrl)
print(BICmodel)
#RMSE = 19.83217
#MAE = 14.75242
#RSquared = 0.4819337
```

```{r Cross Validation results, echo=FALSE}
crossVal=as.table(t(matrix(data=c(19.83,14.75,0.4819,20.22,15.19,0.4616),nrow=2,ncol=3,byrow = TRUE)))
colnames(crossVal)=c('BIC stepwise model','Final model')
rownames(crossVal)=c('Root Mean Squared Error','Mean Absolute Error','R-Squared')
knitr::kable(crossVal, caption = "Errors and goodness of fit measures for first BIC stepwise regression model against final model")
```

As can be seen from the table, the first model is better than the final model in terms of goodness of fit as well as the errors, this is not by a large margin however. The root mean squared error of the final model is only 0.39 higher (1.9%) than the first model. The mean absolute error is only 0.43 higher (2.9%), so the difference is not that large. With respect to the R-Squared values, the difference between them is 0.02, which is also low. As a result, the alterations to the model for simplification and to eliminate collinearity have not adversely affected the model by a large margin. The prediction capabilities of the model are good and the model fits the data well.


## Ridge Regression
Performing a Ridge regression will not contribute to any variable selection but will instead improve the predictive power of the model by shrinking parameter estimates towards 0. 
It should be used in a scenario when the # of parameters is large or when multicollinearity/collinearity exists between variables.
It will increase the bias that exists in the model, which in turn can reduce mean square error by improving variance. Despite this bias the Ridge Regression estimates will have a lower mean square error than least squared estimator.
It will be better to use another penalized likelihood method such as LASSO. However, could be potential to use it after fitting some models to improve predictive power.

## LASSO

Summary: 
Running a LASSO on the transformed data and using the min.lambda from cross validation gives less variables to include (in general it's standard to use the value of lambda which is 1se away). The variables were incidecne rate, median income, percent unemplyed and percent private cvoverage. I like this as well since these cover pretty much all the varibels we might intuitvely expect without using more than one variables measuring eseentially the same thing (ie: percent private and percent publc coverage).
Then extendning this we see an inclusion of a quadratic term for incidence rates. Interaction terms seemed to get messy so I decided not to include any. For this model I used the 1se lambda value. 
This final model appears to have some issues with normality of residuals and maybe slight violations of homoscedascity. I need to explore this further.


PLEASE IGNORE THIS 
I found a mistake in my stuff, I'm working on it atm
```{r}
require(glmnet) 
#In this code I'm using cancer 2 as defined above with the changes that incidence rate is power transformed with a p value of 1.5. I'm going to change these above #but if I haven't when someone is readign this then bear it in mind. ALso included is the region, county and state variables Connor added.

IncidenceSqrd <- cancer2[,3]^2
cancerLASSO <- cancer2[,c(3,4,12,13)]
cancerLASSO <- cbind(cancerLASSO, IncidenceSqrd)

LASSOfit <- glmnet(cancerLASSO, cancer2$deathRate, alpha = 1) #can be used for a plot 
cvLASSO <- cv.glmnet(cancerLASSO, cancer2$deathRate, alpha = 1)

LASSOcandidate <- glmnet(cancerLASSO, cancer2$deathRate, alpha = 1, lambda = cvLASSO$lambda.1se)

LASSOpredicted <- predict(LASSOcandidate, newx = cancerLASSO)
LASSOresiduals <- cancer2$deathRate - LASSOpredicted
#Note here that Union County is making plots look bad
```

## Comparison of models found from Stepwise and LASSO variable selection


## Limitations and recommendations for improvements in future work






















# Appendix

```{r}
#Correlation of employment variables
cancerNoNAs <- na.omit(cancerRaw)
cor(cancerNoNAs$PctUnemployed16_Over,cancerNoNAs$PctEmployed16_Over)
```

```{r,eval=FALSE}
#Cleaning the data
cancer$AvgHouseholdSize[cancer$AvgHouseholdSize <= 0.1] <- cancer$AvgHouseholdSize[cancer$AvgHouseholdSize <= 0.1]*100
cancer <- subset(cancer, select=-c(PctEmployed16_Over)) 
```

Showing fixed AvgHouseholdSize

```{r histogramAvgHouseholdSize included, echo = FALSE, eval = TRUE, message = FALSE, warning = FALSE, fig.asp = 0.4, fig.cap = "Histograms of AvgHouseholdSize after scaling small values"}
par(mfrow = c(1,2))
hist(cancerRaw$AvgHouseholdSize, breaks = 30, main = "Raw AvgHouseholdSize", ylab = "Frequency", xlab = "AvgHouseholdSize")
hist(cancer$AvgHouseholdSize, breaks = 30, main = "Corrected AvgHouseholdSize", ylab = "Frequency", xlab = "AvgHouseholdSize")
```


```{r,eval=FALSE}
#Creating a Region variable, as a factor
cancer2 = separate(cancer,"Geography",c("County","State"),sep = ", ")
cancer2 <- cancer2 %>%
  mutate(Region = case_when(
    State %in% c("New Hampshire","New Jersey","New York","Maine",
                 "Massachusetts","Vermont","Connecticut","Pennsylvania",
                 "Rhode Island") ~ "Northeast",
    State %in% c("Wisconsin","Nebraska","Michigan","Minnesota",
                 "North Dakota","Missouri","Kansas","Ohio",
                 "Indiana","Iowa","Illinois","South Dakota") ~ "Midwest",
    State %in% c("West Virginia","Virginia","North Carolina","Alabama",
                 "Arkansas","Tennessee","Texas","Louisiana",
                 "Maryland","Mississippi","Kentucky","Delaware",
                 "District of Columbia","Florida","Oklahoma","South Carolina",
                 "Georgia") ~ "South",
    State %in% c("Washington","Nevada","New Mexico","California",
                 "Montana","Utah","Colorado","Wyoming",
                 "Oregon","Hawaii","Idaho","Alaska","Arizona") ~ "West"
  ))
cancer2$Region <-  factor(cancer2$Region)
```

```{r,Eeval=FALSE}
#Power transformation UDF
boxcox <- function(x, p){ 
  if (p == 0){
    return(log(x)) 
  }
  else{
    return((x^p - 1)/p)
  } 
}
```





