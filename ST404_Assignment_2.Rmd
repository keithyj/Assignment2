---
title: "ST404_Assignment_2"
output:
  pdf_document: default
fontsize: 11pt
linestretch: 1.5
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	fig.align = "center",
	fig.width = 7,
	message = FALSE,
	include = TRUE,
	warnings = FALSE,
	options(scipen = 999)
)
```

```{r, include = FALSE}
#Loading the data
load('cancer.rdata')
attach(cancer)
#Storing raw data
cancerRaw <- cancer
#Loading packages
library(tidyr)
library(dplyr)
library(olsrr)
library(car)
library(glmnet)
library(caret)
```
# Findings

# Statistical Methodology

Our findings are based off the preliminary EDA of the US Cancer Dataset.

## Cleaning The Data
### Missing Values
As declared in the EDA, missing values exist in PctEmployed16_Over and extremely low values exist in AvgHouseholdSize. We choose to remove the PctEmployed16_Over column in order to keep all 3047 counties within the dataset. 152 PctEmplyed16_Over values were missing which is a sizeable proportion in a dataset this size. Another option was to fill in the missing values by predicting them using a linear model created by the other variables. However, the variable PctUnemployed16_Over is very strongly negatively correlated with PctEmployed16_Over with a Pearson correlation coefficient of -0.65. Therefore, on balance, it is better to remove PctEmployed16_Over entirely to avoid multicollinearity with PctUnemployed16_Over, and to avoid increasing the variance of the model by performing further predictions of the missing values. We will clean the data in the following way:

* Scale the values between 0 and 0.1 in AvgHouseholdSize by 100.
* Remove the PctEmployed16_Over column containing missing values.

``` {r ScaleColumn, include = FALSE}
# Multiply values less than 0.1 by 100 and replace column
cancer$AvgHouseholdSize[cancer$AvgHouseholdSize <= 0.1] <- cancer$AvgHouseholdSize[cancer$AvgHouseholdSize <= 0.1]*100
```

``` {r RemoveMissingValuesColumn, include = FALSE}
cancer <- subset(cancer, select=-c(PctEmployed16_Over, binnedInc))
```
We also add a variable describing the region in which the county is in. We split the US into 4 regions defined by the Census Bureau-designated regions and divisions **Reference Needed**. This division was chosen as it is the most common division for the purposes of data analysis since it groups states into groups with similar geographical features and cultures among other factors. These regions are Northeast, Midwest, South and West.

``` {r region variable, include=FALSE}
cancer = separate(cancer,"Geography",c("County","State"),sep = ", ")
cancer <- cancer %>%
  mutate(Region = case_when(
    State %in% c("New Hampshire","New Jersey","New York","Maine","Massachusetts","Vermont","Connecticut","Pennsylvania","Rhode Island") ~ "Northeast",
    State %in% c("Wisconsin","Nebraska","Michigan","Minnesota","North Dakota","Missouri","Kansas","Ohio","Indiana","Iowa","Illinois","South Dakota") ~ "Midwest",
    State %in% c("West Virginia","Virginia","North Carolina","Alabama","Arkansas","Tennessee","Texas","Louisiana","Maryland","Mississippi","Kentucky","Delaware","District of Columbia","Florida","Oklahoma","South Carolina","Georgia") ~ "South",
    State %in% c("Washington","Nevada","New Mexico","California","Montana","Utah","Colorado","Wyoming","Oregon","Hawaii","Idaho","Alaska","Arizona") ~ "West"
  ))
cancer$Region <-  factor(cancer$Region)

```

## Modelling Flow
Now that we have established the missing values and corrected these lets first look at the scatter plots of the data and fit a model based on observations of these. We looked at the correlation in our initial EDA and so we will use this to fit a model. 

``` {r redundancy, include = FALSE}
# Look at scatter plots and compare weak positive correlations to the correlations shown in original plots.
par(mfrow = c(2,3))
with(cancer, scatter.smooth(deathRate~Edu18_24, ylab = "deathRate", xlab = "Edu18_24", lpars = list(col = "red", lwd = 2), main = "deathRate against Edu18_24", cex.main = 1))
with(cancer, scatter.smooth(deathRate~PctMarriedHouseholds, ylab = "deathRate", xlab = "PctMarriedHouseholds", lpars = list(col = "red", lwd = 2), main = "deathRate against PctMarriedHouseholds", cex.main = 1))
with(cancer, scatter.smooth(deathRate~PctBlack, ylab = "deathRate", xlab = "PctBlack", lpars = list(col = "red", lwd = 2), main = "deathRate against PctBlack", cex.main = 1))
with(cancer, scatter.smooth(deathRate~PctPrivateCoverage, ylab = "deathRate", xlab = "PctPrivateCoverage", lpars = list(col = "red", lwd = 2), main = "deathRate against PctPrivateCoverage", cex.main = 1))
with(cancer, scatter.smooth(deathRate~PctPublicCoverage, ylab = "deathRate", xlab = "PctPublicCoverage", lpars = list(col = "red", lwd = 2), main = "deathRate against PctPublicCoverage", cex.main = 1))
with(cancer, scatter.smooth(deathRate~povertyPercent, ylab = "deathRate", xlab = "povertyPercent", lpars = list(col = "red", lwd = 2), main = "deathRate against povertyPercent", cex.main = 1))
with(cancer, scatter.smooth(deathRate~medIncome, ylab = "deathRate", xlab = "medIncome", lpars = list(col = "red", lwd = 2), main = "deathRate against medIncome", cex.main = 1))
with(cancer, scatter.smooth(deathRate~incidenceRate, ylab = "deathRate", xlab = "incidenceRate", lpars = list(col = "red", lwd = 2), main = "deathRate against incidenceRate", cex.main = 1))
with(cancer, scatter.smooth(deathRate~Region, ylab = "deathRate", xlab = "Region", lpars = list(col = "red", lwd = 2), main = "deathRate against Region", cex.main = 1))
with(cancer, scatter.smooth(deathRate~MedianAgeMale, ylab = "deathRate", xlab = "MedianAgeMale", lpars = list(col = "red", lwd = 2), main = "deathRate against MedianAgeMale", cex.main = 1))
with(cancer, scatter.smooth(deathRate~MedianAgeFemale, ylab = "deathRate", xlab = "MedianAgeFemale", lpars = list(col = "red", lwd = 2), main = "deathRate against MedianAgeFemale", cex.main = 1))
with(cancer, scatter.smooth(deathRate~AvgHouseholdSize, ylab = "deathRate", xlab = "AvgHouseholdSize", lpars = list(col = "red", lwd = 2), main = "deathRate against AvgHouseholdSize", cex.main = 1))
with(cancer, scatter.smooth(deathRate~PercentMarried, ylab = "deathRate", xlab = "PercentMarried", lpars = list(col = "red", lwd = 2), main = "deathRate against PercentMarried", cex.main = 1))
with(cancer, scatter.smooth(deathRate~PctEmpPrivCoverage, ylab = "deathRate", xlab = "PctEmpPrivCoverage", lpars = list(col = "red", lwd = 2), main = "deathRate against PctEmpPrivCoverage", cex.main = 1))
with(cancer, scatter.smooth(deathRate~PctUnemployed16_Over, ylab = "deathRate", xlab = "PctUnemployed16_Over", lpars = list(col = "red", lwd = 2), main = "deathRate against PctUnemployed16_Over", cex.main = 1))
```

The variables we will not include in the initial model will be those that show no relationship with deathRate as they will not be significant. From the scatter plots we are unable to see whether this is the case, so instead we will fit an initial model of the data set and check significance.

``` {r adequacy, include = FALSE}
Model0 <- lm(deathRate ~ Region + incidenceRate + medIncome + povertyPercent + MedianAgeMale + MedianAgeFemale + AvgHouseholdSize + PercentMarried + PctBlack + PctMarriedHouseholds + PctPrivateCoverage + PctPublicCoverage + PctEmpPrivCoverage + PctUnemployed16_Over + Edu18_24, data = cancer)

summary(Model0)
```
We test the null hypothesis that there is no linear association between deathRate and povertyPercent, deathRate and medianAgeMale, and deathRate and PercentMarried. We check the p values for these which show 0.656,  0.936, 0.360, and 0.141, hence we fail to reject the null hypothesis. We will not remove any of these variables from the model at this moment in time as there could be underlying issues as to why they are not significant, such as multicollinearity. 

We will do this by assessing the adequacy of the model. We want to know if the functional form of the model is correct. We will look at the following plots to assess this:


``` {r, echo = FALSE, eval = FALSE}
par(mfrow=c(2,2))
plot(Model0)
```

We can see from the plots that the variance in errors is not constant and non-linearity. We also notice from our intitial EDA that some individual predictor variables are not linear and show signs of heteroscedascity, so we will need to perform some transformations of some variables. 

We first transform the individual predictor variables to satisfy homoscedascity, these include the following transformations:

* log(PctBlack$+0.1$)
* incidenceRate^1.5
* sqrt(PctUnemployed16_Over)
* log(medianIncome)

```{r, include=FALSE}
#Power transformation UDF
boxcox <- function(x, p){ 
  if (p == 0){
    return(log(x)) 
  }
  else{
    return((x^p - 1)/p)
  } 
}
```

``` {r transformations, include = FALSE}
cancer5=cancer

cancer5$incidenceRate=boxcox(cancer5$incidenceRate, p = 1.5)
cancer5$medIncome=log(cancer5$medIncome)
cancer5$PctUnemployed16_Over=boxcox(cancer5$PctUnemployed16_Over, p = 0.5)
cancer5$PctBlack=log(cancer5$PctBlack+0.1)

```

``` {r checkingassumptions, echo = FALSE, eval = TRUE}
#Check for any updated heteroscedascity and linearity
par(mfrow=c(2,2))
plot(lm(deathRate ~ incidenceRate, data = cancer5), 1)
plot(lm(deathRate ~ incidenceRate, data = cancer5), 3)
plot(lm(deathRate ~ medIncome, data = cancer5), 1)
plot(lm(deathRate ~ medIncome, data = cancer5), 3)
plot(lm(deathRate ~ PctBlack, data = cancer5), 1)
plot(lm(deathRate ~ PctBlack, data = cancer5), 3)
plot(lm(deathRate ~ PctUnemployed16_Over, data = cancer5), 1)
plot(lm(deathRate ~ PctUnemployed16_Over, data = cancer5), 3)


# Notice heteroscedacity has decreased for incidence rate, homoscedasticity exists for pctBlack and medIncome.
# Linearity for PctBlack, linearity for medIncome, incidenceRate is linear until a certain point
```

These transformations show evidence of homoscedascity and linearity thus we will update the new model with the transformed variables.

``` {r adequacy2, echo = FALSE, eval = TRUE}
Model1 <- lm(deathRate ~ Region + incidenceRate + medIncome + povertyPercent + MedianAgeMale + MedianAgeFemale + AvgHouseholdSize + PercentMarried + PctBlack + PctMarriedHouseholds + PctPrivateCoverage + PctPublicCoverage + PctEmpPrivCoverage + PctUnemployed16_Over + Edu18_24, data = cancer5)
par(mfrow=c(2,2))
plot(Model1)
```
We can see that the new model is linear and heteroscedascity has been reduced, but it still exists. We will go on to calculate if any outliers exist and if they have high leverage.

Several predictor variables may have observations that can be considered for removal if they are outliers with a high influence. 

``` {r CheckOutliers, echo = FALSE, eval = TRUE}
outlierTest(Model1)
# Shows some outliers exist, namely points 1093, 2713, 1941, 1220, 2645, 1058
par(mfrow=c(1,2))
ols_plot_resid_lev(Model1)
# This plot shows that the observations that are outliers with high leverage that we declare highly influencial are 2713, 1941 and 1059.
par(mfrow=c(2,2))
ols_plot_dfbetas(Model1)
```
Observations 2714, 2682, 166, 1059, 1942, 1094 and 282 appear to be problematic appearing as high leverage, high outliers and also being influencial observations as shown by the DFBETAS > 0.3 we will declare this to be highly influencial and so remove these observations from our model.

``` {r removeoutliers, include = FALSE}
library(dplyr)
cancer5 <-  slice(cancer5, -c(2714, 2628, 166, 1059, 1942, 1094, 282))
```
The new model is seen as below.
``` {r Model3, echo = FALSE, eval = TRUE}
Model3 <- lm(deathRate ~ Region + incidenceRate + medIncome + povertyPercent + MedianAgeMale + MedianAgeFemale + AvgHouseholdSize + PercentMarried + PctBlack + PctMarriedHouseholds + PctPrivateCoverage + PctPublicCoverage + PctEmpPrivCoverage + PctUnemployed16_Over + Edu18_24, data = cancer5)
par(mfrow=c(2,2))
plot(Model3)
```

``` {r checklargesample, include = FALSE}
summary(cancer5)
#Shows 3040 observations 
```
We have 3040 observations in our sample space, so this is large. This means that we can pay less attention to the Q-Q plot as the errors being normally distributed is not a major concern.

We will now check to see if deathRate highly correlates to any of the explanatory variables by analysing the model's summary statistics.
``` {r summarymod3, include = FALSE}
summary(Model3)
```
Observing this model, we can see the F statistic is high with a p-value <0.05. So, we can determine that our response variable has a relationship with at least one of the variables in the model and we can further test which model is the optimal model.

Let's check to see if there is any redundancy in the model. We will spot this by comparing the summary statistics output above with the scatter plots we saw in our EDA. 

``` {r multicolliearity2, include = FALSE}
# Look at scatter plots and compare weak positive correlations to the correlations shown in original plots.
par(mfrow = c(2,3))
with(cancer5, scatter.smooth(deathRate~Edu18_24, ylab = "deathRate", xlab = "Edu18_24", lpars = list(col = "red", lwd = 2), main = "deathRate against Edu18_24", cex.main = 1))
with(cancer5, scatter.smooth(deathRate~PctMarriedHouseholds, ylab = "deathRate", xlab = "PctMarriedHouseholds", lpars = list(col = "red", lwd = 2), main = "deathRate against PctMarriedHouseholds", cex.main = 1))
with(cancer5, scatter.smooth(deathRate~PctBlack, ylab = "deathRate", xlab = "PctBlack", lpars = list(col = "red", lwd = 2), main = "deathRate against PctBlack", cex.main = 1))
with(cancer5, scatter.smooth(deathRate~PctPrivateCoverage, ylab = "deathRate", xlab = "PctPrivateCoverage", lpars = list(col = "red", lwd = 2), main = "deathRate against PctPrivateCoverage", cex.main = 1))
with(cancer5, scatter.smooth(deathRate~PctPublicCoverage, ylab = "deathRate", xlab = "PctPublicCoverage", lpars = list(col = "red", lwd = 2), main = "deathRate against PctPublicCoverage", cex.main = 1))
with(cancer5, scatter.smooth(deathRate~povertyPercent, ylab = "deathRate", xlab = "povertyPercent", lpars = list(col = "red", lwd = 2), main = "deathRate against povertyPercent", cex.main = 1))
with(cancer5, scatter.smooth(deathRate~medIncome, ylab = "deathRate", xlab = "medIncome", lpars = list(col = "red", lwd = 2), main = "deathRate against medIncome", cex.main = 1))
with(cancer5, scatter.smooth(deathRate~incidenceRate, ylab = "deathRate", xlab = "incidenceRate", lpars = list(col = "red", lwd = 2), main = "deathRate against incidenceRate", cex.main = 1))
with(cancer5, scatter.smooth(deathRate~Region, ylab = "deathRate", xlab = "Region", lpars = list(col = "red", lwd = 2), main = "deathRate against Region", cex.main = 1))
with(cancer5, scatter.smooth(deathRate~MedianAgeMale, ylab = "deathRate", xlab = "MedianAgeMale", lpars = list(col = "red", lwd = 2), main = "deathRate against MedianAgeMale", cex.main = 1))
with(cancer5, scatter.smooth(deathRate~MedianAgeFemale, ylab = "deathRate", xlab = "MedianAgeFemale", lpars = list(col = "red", lwd = 2), main = "deathRate against MedianAgeFemale", cex.main = 1))
with(cancer5, scatter.smooth(deathRate~AvgHouseholdSize, ylab = "deathRate", xlab = "AvgHouseholdSize", lpars = list(col = "red", lwd = 2), main = "deathRate against AvgHouseholdSize", cex.main = 1))
with(cancer5, scatter.smooth(deathRate~PercentMarried, ylab = "deathRate", xlab = "PercentMarried", lpars = list(col = "red", lwd = 2), main = "deathRate against PercentMarried", cex.main = 1))
with(cancer5, scatter.smooth(deathRate~PctEmpPrivCoverage, ylab = "deathRate", xlab = "PctEmpPrivCoverage", lpars = list(col = "red", lwd = 2), main = "deathRate against PctEmpPrivCoverage", cex.main = 1))
with(cancer5, scatter.smooth(deathRate~PctUnemployed16_Over, ylab = "deathRate", xlab = "PctUnemployed16_Over", lpars = list(col = "red", lwd = 2), main = "deathRate against PctUnemployed16_Over", cex.main = 1))
```
The scatter plots show a negative correlation between deathRate and PctMarriedHouseholds and PctEmpPrivCoverage, which are not negative in the summary of the model. Moreover, the estimate is negative for PovertyPercent and PctBlack in the summary statistics but clearly not in the plot. This shows there exists redundancy and can be explained potentially by any collinearity. The other redundancy we see is that MedianAgeMale and MedianAgeFemale plot against deathRate does not correlate with the estimate in the summary statistic. These redundancies could be caused by multicollinearity.

``` {r multicollinearity, include = FALSE}
library(olsrr)
ols_coll_diag(Model3)
#Look for low tolerance, VIF > 5 and condition index > 30
```
This shows that PctMarriedHouseholds, PctPrivateCoverage, PercentMarried, povertyPercent and medIncome all had VIF >5. When looking at the condition index's above 30, the variance decomposition proportion above 0.3, which is said to be the cut off **Reference Needed** , is PctEmpPrivCoverage when condition index is 45.36. When the condition index is 48, PctPublicCoverage is an issue. When the condition index is 78 PctPrivateCoverage and Edu19_24 are issues. When the condition index is 123, issues lie with MedianAgeMale and Female. When condition index is 159, issues lie with AvgHouseholdSize, PctMarried and PctMarriedHouseholds. When condition index is 262, the issues are with when condition index is 262.44: incidenceRate is an issue. povertyPercent, intercept and medIncome when the condition index is 547.2. 

We can see that the variables that are, to some degree, multicollinear are: povertyPercent and medIncome, AvgHouseholdSize, PercentMarried and PctMarriedHouseholds, MedianAgeMale and MedianAgeFemale, and PctPrivateCoverage and Edu19_24.

```{r, include=FALSE}
#defining boxcox function
boxcox <- function(x, p){ 
  if (p == 0){
    return(log(x)) 
  }
  else{
    return((x^p - 1)/p)
  } 
}
```
## Modelling and Variable Selection

### Stepwise Regression

**Initial Ideas**

Before attempting to fit any models or try stepwise regression, we looked at what variables we thought would be most important in determining the death rate within a county. We came to the conclusion that incidence rate would be the most important. Median income within a region as well as the type of health coverage in that region would be important. The median age in a county may also affect the death rate. Additionally, an intercept may need to be excluded as it makes no sense to predict the death rate to be greater than zero when all variables (particularly incidence rate) are equal to zero.

We also looked at combinations of variables that could be problematic if they are in the same model. This was based heavily on the correlation between variables. The correlation values suggest that we should not include more than one variable explaining the type of health coverage, both of `povertyPercent` and `medIncome`, both of the median age variables or both of the marriage rate variables. 

```{r, include=FALSE,eval=FALSE,message=FALSE}
require(corrplot)
correlations1 <- cor(cancer[,c(2,3,5:17)],method='spearman')
corrplot(correlations1,method='square',title='Spearman Correlation plot', tl.col="black", tl.cex=0.6,mar=c(1,1,1,1))
```

**Without transformations**

Initially we attempted stepwise regression for the data set without any transformations. Despite using both AIC and BIC, we had issues with there being too many parameters (10-12 parameters) and this meant that the model was not simple. As a result we chose to apply the transformations and see if that gave better results.

**With transformations**

First we apply the transformations and create a separate transformed dataset, then we define the bounds of the complexity of our model. Then we do the same as above and perform stepwise regression on this dataset using both AIC and BIC.



**AIC**

We first attempted to use AIC but this returned a model with 9 variables in it as well as an intercept term. Despite the fact this model is a good explanation of the data, it is far from simple and would be difficult to interpret properly. As a result we opted to go forward using BIC as this gave a heavier penalty for having more variables.

```{r Define null and full models, include=FALSE}
fullTransModel=lm(deathRate ~ Edu18_24 + medIncome + PctMarriedHouseholds + 
                    PctBlack + PctPublicCoverage + PctPrivateCoverage + 
                    PctUnemployed16_Over + PercentMarried + povertyPercent + 
                    incidenceRate + Region,
                  data = cancer5)
nullTransModel=lm(deathRate~1,data=cancer5)
```


```{r AIC Stepwise, include=FALSE}
AICmodel1 = step(nullTransModel,direction='both',scope=formula(fullTransModel))
ols_coll_diag(AICmodel1)
```

**BIC**

```{r BIC Stepwise 1, include=FALSE}
BICmodel1 = step(nullTransModel,direction='both',scope=formula(fullTransModel), k=log(3040))
ols_coll_diag(BICmodel1)
summary(BICmodel1)
```

Our first model for the BIC stepwise regression model had 6 variables in it which is a much easier to interpret than what the AIC suggested. However, there were issues with collinearity here with a condition number of 264. This is too high to be able to justify using this model and so we looked to find which variables were problematic. The model suggested that it was the median income and the intercept term. We initially thought that having an intercept term may not be the best approach going forward and so we removed it at this point.

```{r BIC Stepwise 2, include=FALSE}
fullTransModel=lm(deathRate ~ 0 + Edu18_24 + medIncome + PctMarriedHouseholds + 
                    PctBlack + PctPublicCoverage + PctPrivateCoverage + 
                    PctUnemployed16_Over + PercentMarried + povertyPercent + 
                    incidenceRate + Region,
                  data = cancer5)
nullTransModel=lm(deathRate~0,data=cancer5)

BICmodel2 = step(nullTransModel,direction='both',scope=formula(fullTransModel), k=log(3040))
ols_coll_diag(BICmodel2)
summary(BICmodel2)
```

Once we had removed the intercept term we found that there were still collinearity issues but this time it was between the median income, education coefficient and regions. Median income appeared to us to be a common theme and there was also the poverty percent which was not being used, which is also a measure of wealth, so there were alternatives. As a result we removed median income from our full model and looked at if the collinearity was solved then.

```{r BIC Stepwise 3, include=FALSE}
fullTransModel=lm(deathRate ~ 0 + Edu18_24 + PctMarriedHouseholds + 
                    PctBlack + PctPublicCoverage + PctPrivateCoverage + 
                    PctUnemployed16_Over + PercentMarried + povertyPercent + 
                    incidenceRate + Region,
                  data = cancer5)

BICmodel3 = step(nullTransModel,direction='both',scope=formula(fullTransModel), k=log(3040))
ols_coll_diag(BICmodel3)
summary(BICmodel3)
```

While the condition number had decreased significantly, it was still 77, which is too large. At this point the problematic variables were the regions, percentage of married households and the education coefficient. The education coefficient and regions were an issue before as well so we decided one of them was best to remove form the full model. We decided that, of the two, we would remove the regions since if a region generally has a higher death rate then it usually has a higher incidence rate too and so part of the region variable is explained by the incidence rate. As a result, we removed the regions from our full model and repeated the stepwise regression again.

```{r BIC Stepwise 4, include=FALSE}
fullTransModel=lm(deathRate ~ 0 + Edu18_24 + PctMarriedHouseholds + 
                    PctBlack + PctPublicCoverage + PctPrivateCoverage + 
                    PctUnemployed16_Over + PercentMarried + povertyPercent + 
                    incidenceRate,
                  data = cancer5)

BICmodel4 = step(nullTransModel,direction='both',scope=formula(fullTransModel), k=log(3040))
ols_coll_diag(BICmodel4)
summary(BICmodel4)
```

After analysing the condition numbers and the VIFs from this model we were able to conclude that collinearity was no longer an issue. The condition number was below 19 and the VIFs were all below 3.5. Consequently, we decided that we could use this model as the model selected by stepwise regression.

**Interpretation**

The model suggested by stepwise regression is shown below.

```{r Model Table, echo=FALSE}
name=c('incidenceRate','povertyPercent','PctUnemployed16_Over',
       'PctPublicCoverage','PctBlack','PercentMarried')
transformation=c('incidenceRate^(1.5)','None','PctUnemployed19_Over^(0.5)','None','log(PctBlack+0.1)',
                'None')
pVals=c('<2e-16','<1.7e-13','4.1e-11','<2e-16','<2e-16','1.0e-11')
beta=c('0.00740','0.527','5.46','1.55','0.884','1.95')
modelTable=as.table(matrix(data=c(transformation,pVals,beta),nrow=6,ncol=3))
rownames(modelTable)=name
colnames(modelTable)=c('Transformation','p-value','Parameter estimate')
knitr::kable(modelTable, caption = "Information on the final model")
```

This model shows, for incidence rate, the value of the parameter estimate is 0.00740. Since the variable is raised to the power of 1.5, this means that if the incidence rate quadruples, then the death rate will increase by 8 times. For example, if the incidence rate changes from 100 to 400 cases per 100000 people, then the death rate will increase from 7.4 to 59.2, if all other variables are equal to 0. Another transformed variable is `PctBlack`, for this variable a parameter estimate of -1.03 means that if the percentage of black population increases then the death rate will decrease. The exact amount of the death rate decrease is dependent on what your percentage increases from and to but the marginal decrease in death rate also decreases as the percentage of black population increases. For example an observed increase in black population from 1% to 2% will result in a larger decrease in death rate than 50% to 51%. The last transformed variable is `PctUnemployed16_Over`, it has a coefficient of 5.46 and it is raised to the power of $\frac{1}{2}$. This means that if the unemployment rate quadruples  then the death rate will double. For example, if the unemployment rate goes from 5% to 20% then the death rate will go from 12.2 to 24.4, if all other variables are equal to 0.

The other variables in the model are not transformed. The percentage of people with public coverage, the percentage of people in poverty and the percentage of people married have a coefficient of 0.527, 1.55 and 0.884 respectively. This means that for every percentage increase in the amount of people with public coverage, in poverty and that are married the death rate increases by 0.527, 1.55 and 0.884 respectively, assuming all other variables stay the same.

All of the variables also have low p-values, they are all significant at the 0.1% significance level which means all of their inclusions in the model are justified. Finally, the F-statistic for the model has a value of 37210 on 6 and 3034 degrees of freedom. This is a high value for the F-statistic and it has an extremely low p-value, less than 2.2e-16, this confirms that the model is a much better fit for the data than a model with just an intercept.

**Cross validation**

To determine how well the model predicts values we use leave one out cross validation (LOOCV). This is included in the package `caret`. We ran this on the model suggested by BIC stepwise regression and the summary is given in the table below.

```{r Cross validation, echo=FALSE}
crossVal=as.table(t(matrix(data=c(20.94,0.4228,16.08),nrow=1,ncol=3,byrow = TRUE)))
colnames(crossVal)=c('BIC stepwise model')
rownames(crossVal)=c('Root Mean Squared Error','Mean Absolute Error','R-Squared')
knitr::kable(crossVal, caption = "Errors and goodness of fit measures for BIC stepwise regression model")
```



## Ridge Regression
Performing a Ridge regression will not contribute to any variable selection but will instead improve the predictive power of the model by shrinking parameter estimates towards 0. 
It should be used in a scenario when the # of parameters is large or when multicollinearity/collinearity exists between variables.
It will increase the bias that exists in the model, which in turn can reduce mean square error by improving variance. Despite this bias the Ridge Regression estimates will have a lower mean square error than least squared estimator.
It will be better to use another penalized likelihood method such as LASSO. However, could be potential to use it after fitting some models to improve predictive power.

## LASSO

Note: might want to say ran an LAR algorithim here, I think that might be what it is called.

Running a LASSO on our data with transformation as outlined above. By using cross validation we get the value of lambda minimising the mean squared error as (fill). However as illustrated in figure ? we can see that this value of lambda includes all coeficents in our model. This would make our model difficult to interpret so we instead consider the value of lambda which is 1se away (fill). Using this more strict restriction term shrinks the following variables to zero: (fill). 

### Put the coefficents plot here with abline used for the values of lambda also include a legend ###


---Extending the LASSO---
In attempting to extend our existing LASSO model we first included interaction terms for variables with high correlation. This included (fill). However by including these variables we ran into more issues with the variables being selected. Therefore we chose to omit the inclusion of interaction terms. With respect to higher order terms we found that the only quadratic terms being included would shrink their linear ocunterpart to zero. Therefore we also decided to omit higher order terms for independent variables from the model.

---Evaluating the goodness of this model--- 
The R squared value for this model was (fill).
The adjusted R Squared value was (fill)
The variance inflation factor for the variables included did not exceed 2, suggesting no issuees with multicolinearity. 
Residuals are homoscedastic and uncorrelated. However the residuals did not demonstrate a normal distribution. 



```{r, eval = FALSE}

cancer3 <- cancerRaw
cancer3 <- cancer3[-c(1489,2713,1941,1490),]

cancer3 = separate(cancer3,"Geography",c("County","State"),sep = ", ")
cancer3 <- cancer3 %>%
  mutate(Region = case_when(
    State %in% c("New Hampshire","New Jersey","New York","Maine","Massachusetts","Vermont","Connecticut","Pennsylvania","Rhode Island") ~ "Northeast",
    State %in% c("Wisconsin","Nebraska","Michigan","Minnesota","North Dakota","Missouri","Kansas","Ohio","Indiana","Iowa","Illinois","South Dakota") ~ "Midwest",
    State %in% c("West Virginia","Virginia","North Carolina","Alabama","Arkansas","Tennessee","Texas","Louisiana","Maryland","Mississippi","Kentucky","Delaware","District of Columbia","Florida","Oklahoma","South Carolina","Georgia") ~ "South",
    State %in% c("Washington","Nevada","New Mexico","California","Montana","Utah","Colorado","Wyoming","Oregon","Hawaii","Idaho","Alaska","Arizona") ~ "West"
  ))
cancer3$Region <-  factor(cancer3$Region)

cancerLASSO <- cancer3[,-c(1,2,5,11,19)]

cancerLASSO$incidenceRate <- boxcox(cancerLASSO$incidenceRate, p = 1.5)
cancerLASSO$PctBlack <- log(cancerLASSO$PctBlack + 0.1 )

cancerLASSO <- model.matrix(~., data = cancerLASSO)

LASSO <- glmnet(cancerLASSO, cancer3$deathRate, alpha = 1) 
cvLASSO <- cv.glmnet(cancerLASSO, cancer3$deathRate, alpha = 1)
LASSOmodel <- glmnet(cancerLASSO, cancer3$deathRate, alpha = 1, lambda = cvLASSO$lambda.1se)

LASSOpredicted <- predict(LASSOmodel, newx = cancerLASSO)
LASSOresiduals <- cancer3$deathRate - LASSOpredicted

LASSO_Rsquared <- 1 - (sum(LASSOresiduals^2)/sum((cancer$deathRate - mean(cancer$deathRate))^2))

```

## Comparison of models found from Stepwise and LASSO variable selection


## Limitations and recommendations for improvements in future work

One limitation of this data is that the populations in each county are different. However, our model puts equal weighting on each county. To gain a more realistic model, perhaps it would be valuable to give different weightings to counties depending on their population.




















# Appendix

```{r}
#Correlation of employment variables
cancerNoNAs <- na.omit(cancerRaw)
cor(cancerNoNAs$PctUnemployed16_Over,cancerNoNAs$PctEmployed16_Over)
```

```{r,eval=FALSE}
#Cleaning the data
cancer$AvgHouseholdSize[cancer$AvgHouseholdSize <= 0.1] <- cancer$AvgHouseholdSize[cancer$AvgHouseholdSize <= 0.1]*100
cancer <- subset(cancer, select=-c(PctEmployed16_Over)) 
```

Showing fixed AvgHouseholdSize

```{r histogramAvgHouseholdSize included, echo = FALSE, eval = TRUE, message = FALSE, warning = FALSE, fig.asp = 0.4, fig.cap = "Histograms of AvgHouseholdSize after scaling small values"}
par(mfrow = c(1,2))
hist(cancerRaw$AvgHouseholdSize, breaks = 30, main = "Raw AvgHouseholdSize", ylab = "Frequency", xlab = "AvgHouseholdSize")
hist(cancer$AvgHouseholdSize, breaks = 30, main = "Corrected AvgHouseholdSize", ylab = "Frequency", xlab = "AvgHouseholdSize")
```


```{r,eval=FALSE}
#Creating a Region variable, as a factor
cancer2 = separate(cancer,"Geography",c("County","State"),sep = ", ")
cancer2 <- cancer2 %>%
  mutate(Region = case_when(
    State %in% c("New Hampshire","New Jersey","New York","Maine",
                 "Massachusetts","Vermont","Connecticut","Pennsylvania",
                 "Rhode Island") ~ "Northeast",
    State %in% c("Wisconsin","Nebraska","Michigan","Minnesota",
                 "North Dakota","Missouri","Kansas","Ohio",
                 "Indiana","Iowa","Illinois","South Dakota") ~ "Midwest",
    State %in% c("West Virginia","Virginia","North Carolina","Alabama",
                 "Arkansas","Tennessee","Texas","Louisiana",
                 "Maryland","Mississippi","Kentucky","Delaware",
                 "District of Columbia","Florida","Oklahoma","South Carolina",
                 "Georgia") ~ "South",
    State %in% c("Washington","Nevada","New Mexico","California",
                 "Montana","Utah","Colorado","Wyoming",
                 "Oregon","Hawaii","Idaho","Alaska","Arizona") ~ "West"
  ))
cancer2$Region <-  factor(cancer2$Region)
```

```{r, eval=FALSE}
#Power transformation UDF
boxcox <- function(x, p){ 
  if (p == 0){
    return(log(x)) 
  }
  else{
    return((x^p - 1)/p)
  } 
}
```



