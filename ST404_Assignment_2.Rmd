---
title: "ST404_Assignment_2"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r, include = FALSE}
load("cancer.RData")
attach(cancer)
```
# Findings

# Statistical Methodology

Our findings are based off the preliminary EDA of the US Cancer Dataset.

## Cleaning The Data
### Missing Values
As declared in the EDA, missing values exist in PctEmployed16_Over and extremely low values exist in AvgHouseholdSize. We will clean the data in the following way:

* Multiply (Scale?) the values between 0 and 0.1 in AvgHouseholdSize by 100. (Show in Appendix a fixed histogram?)
* Remove the PctEmployed16_Over column containing missing values. (Explanation of why?)

``` {r ScaleColumn, echo = FALSE}
# Multiply values less than 1 by 100 and replace column
AvgHouseholdSize[AvgHouseholdSize <= 0.1] <- AvgHouseholdSize[AvgHouseholdSize <= 0.1]*100
```
``` {r RemoveMissingValuesColumn, echo = FALSE}
cancer <- subset(cancer, select=-c(PctEmployed16_Over))
```

### Outliers
Several predictor variables may have outliers that can be considered for removal if they have a high leverage, specifically, incidenceRate. We will look at Cook's distance to declare if any of these outliers have significant influence.

``` {r CheckOutliers, echo = FALSE}
par(mfrow=c(1,2))
# Plot the cook's distance
plot(lm(deathRate ~ incidenceRate,data=cancer),4)
plot(lm(deathRate ~ .,data=cancer[-c(1,4)]),4)
```
Do we remove the Williamsburg one?

## Transformations

Due to the large quantity of variables, it is important to keep the model simple so that it is easy to interpret and maintains explanatory power.

Firstly, looking at skewed variables, we could perform transformations of the left-skewed variables PctPrivateCoverage and PercentMarried, and the right-skewed variables PctBlack (after a shift of 0.05 due to some values being 0), IncidenceRate, medIncome, AvgHouseholdSize, PctUnemployed16_Over and PovertyPercent. This does not totally remove skew from PctBlack but does reduce it substantially. These transformations were discussed in assignment 1 and are listed below.

Secondly, to deal with variables showing signs of heteroscedasticity, we could log transform deathRate, the dependent variable. This includes the variables medianIncome, PctPrivateCoverage, PctEmpPrivCoverage and PctPublicCoverage.

Should we do some tests using diagnosis methods we can now use and decide which ones we actually wish to transform? i.e spreadlevelplot, residual plots etc.


List of potential transformed variables with associated transformation:

* PctPrivateCoverage^2^
* PercentMarried^2^
* log(PctBlack$+1$)
* log(IncidenceRate)
* log(medianIncome)
* log(AvgHouseholdSize)
* PctUnemplyed16_over^$\frac{1}{2}$^
* PovertyPercent^$\frac{1}{5}$
* log(deathRate)

These potential transformations will be completed within the model itself in order for the variable names to retain their true meaning.

## Modelling and Variable Selection
### Stepwise Regression

**Without transformations**

Code that defines the maximum complexity, all variables included, and the minimum complexity, only an intercept term. It then runs stepwise regression using both AIC and BIC to determine the steps.

```{r}
fullModel=lm(deathRate~incidenceRate+medIncome+povertyPercent+MedianAgeMale+
               MedianAgeFemale+AvgHouseholdSize+PercentMarried+
               PctUnemployed16_Over+PctPrivateCoverage+PctEmpPrivCoverage+
               PctPublicCoverage+PctBlack+PctMarriedHouseholds+Edu18_24,data=cancer)
nullModel=lm(deathRate~1,data=cancer)
```

```{r}
aicModel=step(nullModel,direction='both',scope=formula(fullModel),trace=0,k=2)
summary(aicModel)
```

This contains 12 variables, all are significant at the 5% level and 9 of them are significant at the 0.1% level. There are, however, a lot of variables which makes the model harder to interpret, however the predictive power should be good.

```{r}
bicModel=step(fullModel,direction='both',scope=formula(nullModel),trace=0,k=log(3047))
summary(bicModel)
```

This contains 9 variables and so is easier to interpret but the predictive power may not be as good. All but one are significant at the 0.1% level.

**With transformations**

First we apply the transformations and create a separate transformed dataset, then we define the bounds of the complexity of our model. Then we do the same as above and perform stepwise regression on this dataset using both AIC and BIC.

```{r}
cancer2=cancer
cancer2$deathRate=log(deathRate)
cancer2$incidenceRate=log(incidenceRate)
cancer2$medIncome=log(medIncome)
cancer2$AvgHouseholdSize=log(AvgHouseholdSize)
cancer2$PercentMarried=PercentMarried^2
cancer2$PctUnemployed16_Over= sqrt(PctUnemployed16_Over)
cancer2$PctPrivateCoverage= PctPrivateCoverage^2
cancer2$PctBlack = log(PctBlack+1)
fullTransModel=lm(deathRate~incidenceRate+medIncome+povertyPercent+MedianAgeMale+
               MedianAgeFemale+AvgHouseholdSize+PercentMarried+
               PctUnemployed16_Over+PctPrivateCoverage+PctEmpPrivCoverage+
               PctPublicCoverage+PctBlack+PctMarriedHouseholds+Edu18_24,data=cancer2)
nullTransModel=lm(deathRate~1,data=cancer2)
```

```{r}
aicTransModel=step(nullTransModel,direction='both',scope=formula(fullTransModel),trace=0,k=2)
summary(aicTransModel)
```

This model contains 13 variables which is a lot of variables and would make it hard to interpret the model. Predictive power will be good but the complexity needs to be penalised more, so we will use the BIC instead.

```{r}
bicTransModel=step(nullTransModel,direction='both',scope=formula(fullTransModel),trace=0,k=log(3047))
summary(bicTransModel)
```

This produces a model with 7 variables which is a lot more manageable and easy to interpret, it has a standard residual error that is only 0.0005 greater than the model that uses AIC. So it fits the data nearly as well, but there are far fewer variables. This model can even be further simplified because when the full output of the stepwise regression function is printed, the change in the value of the BIC is very little in the last steps compared to the first steps. We can simplify our model further by removing `PctBlack` and `PctPublicCoverage`. This reduces our R-squared value by 0.009 and our adjusted R-squared value by 0.0086 but it also removes two variables which makes the model simpler. All p-values for this model are below 0.0000000004, which is very low.

```{r}
altBICTransModel=lm(deathRate~incidenceRate+medIncome+PctPrivateCoverage+
                 PctEmpPrivCoverage+Edu18_24,data=cancer2)
summary(altBICTransModel)
```

**Multicollinearity**

```{r}
require(olsrr)
summary(altBICTransModel)
ols_coll_diag(altBICTransModel)
```

There are issues here in the condition numbers and the variables that contribute to them. medIncome and incidenceRate variables appear to be the issue. These are not correlated yet they seem to explain the same variance in the deathRate but both are very important in prediction. As a result it is likely best that both remain in the model due to the fact removing one would have a massive impact on the prediction power of our model. 


# Ridge Regression
Performing a Ridge regression will not contribute to any variable selection but will instead improve the predictive power of the model by shrinking parameter estimates towards 0. 
It should be used in a scenario when the # of parameters is large or when multicollinearity/collinearity exists between variables.
It will increase the bias that exists in the model, which in turn can reduce mean square error by improving variance. Despite this bias the Ridge Regression estimates will have a lower mean square error than least squared estimator.
It will be better to use another penalized likelihood method such as LASSO. However, could be potential to use it after fitting some models to improve predictive power.

# LASSO




















# Appendix

Loading the data
```{r}
load("cancer.RData")
attach(cancer)
```


Cleaning the data
```{r}
AvgHouseholdSize[AvgHouseholdSize <= 0.1] <- AvgHouseholdSize[AvgHouseholdSize <= 0.1]*100
cancer <- subset(cancer, select=-c(PctEmployed16_Over)) 
```


