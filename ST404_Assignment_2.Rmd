---
title: "ST404_Assignment_2"
output: 
  pdf_document:
    toc: true
    toc_depth: 2
    number_sections: true
fontsize: 11pt
linestretch: 1.5
bibliography: References404.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	fig.align = "center",
	fig.width = 7,
	message = FALSE,
	include = TRUE,
	warnings = FALSE
)
```

```{r, include = FALSE}
#Loading the data
load('cancer.rdata')
attach(cancer)
#Storing raw data
cancerRaw <- cancer
#Loading packages
library(tidyr)
library(dplyr)
library(olsrr)
library(car)
library(glmnet)
library(caret)
library(corrplot)
```

\pagebreak 

# Findings

All statements made in our findings are supported in our statistical methodology (Section 2) or from previous EDA on the data.

## Data Cleaning

Based on our previous EDA showing 152 missing observations in PctEmployed16_Over, and a high correlation between this variable and PctUnemployed16_Over, we decided to remove the PctEmployed16_Over column and ignore this for our analysis. This decision was favoured over removing the missing observation rows from the dataset as even though the dataset is large, it still contributes to more accurate analysis of the other variables. We also decided to scale the infeasibly small observations of AvgHouseholdSize (those less than 0.1) by 100. In order to account for different death rates found in different states within the US, we divided the US into 4 regions and allocated each county into one of these 4 regions. This variable, named 'Region' will then be used to construct a model. We noticed that medIncome and binnedInc are similar metrics and so we removed binnedInc from the data before we conducted any analysis. This is beacuase binnedInc is given as an interval for each county and so is not feasible to be directly used in the model.   

## Multicollinearity

We notice multicollinearity between medIncome and percentPoverty, marriage and types of insurance cover; median age variables with AvgHouseholdSize; employment statistics with type of insurance cover; marriage statistics with PctBlack. 

## Transformations

Some transformations of the variables were completed in order to fulfil the model assumptions, mainly homoscesastiscity (constant variance of errors) and linearity. Transformations were kept to a minimum in order to keep the model as simple as possible. One of our transformations required us to first scale PctBlack by 0.1. Hence, the transformations we use for building a model are: power transform incidenceRate and PctUnemployed16_Over as follows incidenceRate^1.5 and sqrt(PctUnemployed16_Over), also log transform PctBlack and medIncome.

## Outliers and Influencial Observations

Several observations were identified as having a high leverage and after testing for influencial observations we declared that the following observations would be removed from the data to ensure the model was not impacted by outliers.

\tiny
```{r Outlier Table, echo=FALSE}
names=c('Williamsburg City','Union County','Aleutians West Census Area','De Baca County','North Slope Borough','Chattahoochee County')
tableOutliers=c('282','1490','2714','166','2727','2682')
reasons=c('Incidence rate, Median Age Female, Percent Married, Percent Married Households, Percent Employer Private Coverage','Incidence rate','Median Age Male, Average Household Size','Average Household Size','Percent Married Households','Percent Private Coverage, Percent Employer Private Coverage')
outReasons=as.table(t(matrix(data=c(tableOutliers,reasons),ncol=6,nrow=2,byrow=TRUE)))
rownames(outReasons)=names
colnames(outReasons)=c('Index','Problematic Variable(s)')
knitr::kable(outReasons, caption = "Outlier analysis")
```

## Modelling
\normalsize
We looked for a model that had the following characteristics: simplicity, explanatory power and predictive power. The variable selection methods we used were stepwise variable selection, LASSO regression and ridge regression.

### Stepwise selection

There were issues with collinearity using this method, meaning lots of variables were removed. The model suggested by stepwise regression is shown below.

\tiny
```{r Model Table Step, echo=FALSE,message=FALSE}
library(sylly)
name=c('incidenceRate','povertyPercent','PctUnemployed16_ Over',
       'PctPublicCoverage','PctBlack','PercentMarried')
transformation=c('incidenceRate^(1.5)','None','PctUnemployed16_Over^(0.5)','None','log(PctBlack+0.1)',
                'None')
beta=c('0.00768','1.53','4.71','0.556','2.02','0.858')
interpretation=c("If incidence rate increases by a factor of x, death rate increases by a factor of x^1.5, assuming all other variables are zero. If incidence rate^1.5 increases by 1, death rate increases by 0.00768 assuming all other variables remain constant.", "If poverty percent increases by 1, death rate increases by 1.53, assuming all other variables remain constant.", "If percent unemployed increases by a factor of x, death rate increases by a factor of x^0.5, assuming all other variables are zero. If percent unemplyed^0.5 increases by 1, death rate increases by 4.71 assuming all other variables remain constant.", "If percent public coverage increases by 1, death rate increases by 0.556, assuming all other variables remain constant.", "If (percent black + 0.1) increases by a factor of x, death rate will increase by 2.02log(x), assuming all other variables remain constant.", "If percent married increases by 1, death rate increases by 0.858, assuming all other variables remain constant.")
modelTable=as.table(matrix(data=c(transformation,beta,interpretation),nrow=6,ncol=3))
rownames(modelTable)=name
colnames(modelTable)=c('Transformation','Parameter estimate','Interpretation')
  pander::pander(modelTable, caption = "Information on Stepwise Model", split.cell = 75, split.table = Inf, style = "grid", justify = c("left", "right", "right", "center"), use.hyphening = TRUE)
```

### LASSO regression
\normalsize

Using LASSO variable selection we create a model which includes eight predictor variables including: those in the table below. We expected this as the scatter plots and summary of our initial model suggested they are significant with respect to death rate. with the exception of medIncome. We found this model to have good predictive power. The coefficients for each of these variables are as follows.
\tiny
```{r Model Table LASSO, echo = FALSE}
library(sylly)
name=c('intercept','incidenceRate','medIncome','PctUnemploy16_Over','PctPrivateCoverage',
       'PctPublicCoverage','Edu18_24','RegionNorthaast','RegionSouth','RegionWest')
beta=c('387','0.010','-23.5','2.49','-0.212','0.127','-8.22','-4.61','3.73','-12.4')
transformation=c('intercept','incidenceRate^(1.5)','log(medIncome)','PctUnemployed16_Over^(0.5)','None','None','None','None','None','Npne')
interpretation=c("If all other variabels were equal to zero death rate would be given as 387, we discus this further below","If incidence rate increases by a factor of x, death rate increases by a factor of x^1.5, assuming all other variables are zero. If incidence rate^1.5 increases by 1, death rate increases by 0.01 assuming all other variables remain constant.", "If median income increases by a factor of x, death rate will increase by a factor of -23.54log(x) assuming all other variables remain constant", "If percent unemployed increases by a factor of x, death rate increases by a factor of x^0.5, assuming all other variables are zero. If percent unemplyed^0.5 increases by 1, death rate increases by 2.49 assuming all other variables remain constant.","If percent private coverage increases by 1, death rate will increase by 0.127, assuming all other variables remain constant", "If percent public coverage increases by 1, death rate increases by 0.127, assuming all other variables remain constant.","If education score increases by one, death rate will decrease 8.22, assuming all other variables remain constant", "If a county belongs to a state in the North Ease region then death rate will decrease by 4.61", "If a count belongs to a state in the South region then death rate will increase by 3.73", "If a county belongs to a state in the West region then death rate will decrease by 12.4")       
modelTable=as.table(matrix(data=c(interpretation,beta,interpretation),nrow=10,ncol=3))
rownames(modelTable)=name
colnames(modelTable)=c('Parameter estimate','Transformation','Interpretation')
pander::pander(modelTable, caption = "Information on Stepwise Model", split.cell = 75, split.table = Inf, style = "grid", justify = c("left","right","right","center"), use.hyphening = TRUE)
```
While it may not make sense to have a death rate greater than zero when all our other variables are equal to zero it would also not make sense to have values such as median income and incidence rate equal to zero. So for reasonable values of our parameters the output of this model is also reasonable.

## Final Model
\normalsize
Our final model is produced by the LASSO variable selection method can be summarised in the table below. 

```{r,eval=FALSE,echo=FALSE}
DF <- data.frame(row.names=c(),coefficient=c(),p-value=c())
DF
```

### Limitations
Stepwise will often include more explanatory variables in the model to gain a better value of R^2^, and so explanatory power of the model. This results in a model which is not necesarrily the best at predicting. LASSO also has limitations in that it does not account for multicollinearity, which is significant in our dataset, but this is also an issue in stepwise regression so LASSO is a lot better than stepwise regression. However, in our final model we see variables that are highly correlated with other variables in our dataset, this could be an issue due to some variables being removed by selection but potentially having a better predictive power than the variable included in the model. Moreover, some variables that are not included will ultimately provide greater explanatory to the model.

In the dataset we notice that some counties have significantly lower population than others, for example Alaska compared to Los Angeles **Need Reference**. This is an issue as they all have the same weightings on producing the final model, it would be better to weight these in the dataset accordingly. We also removed the percent of employed 18 and over, due to the missing data and its similarity to the percent of unemployed 18 and over. However, the employed variable with imputed values could be better for the model and show significance to the predictive power of the model.


 
### What are the major determinants of high mortality rates?

Major determinants of the death rate can be calculated by looking at the amount each variable can affect the overall death rate. We look only at the LASSO model because this is our final model. Within the LASSO model, the variable which has the most influence on the overall death rate is incidence rate, followed by median income, followed by the percentage of unemployed population, education coefficient, region and percentage of people with private health coverage, which all have a similar power over the death rate.
 
### Areas of the US with unusually low or high mortality rates that do not conform to the general pattern
 
 ••• STILL NEED TO DO •••
 
We look for outliers within our model, as well as seeing if the counties we removed for having high leverage initially conform to our model and therefore to the general pattern.  

\pagebreak

# Statistical Methodology

Our findings are based off the preliminary EDA of the US Cancer Dataset.

## Cleaning the Data

Many, 152, PctEmployed16_Over observations are missing. We could fill in the missing observations by predicting them using a linear model created by the other variables. However, the variable PctUnemployed16_Over is strongly negatively correlated with PctEmployed16_Over. Therefore, we choose to remove PctEmployed16_Over entirely to avoid any issues with the missing observations and collinearity with PctUnemployed16_Over, and to avoid increasing the variance of the model by performing further predictions of the missing values. We will also scale the values of AvgHouseholdSize between 0 and 0.1 by 100.

``` {r ScaleColumn, include = FALSE}
# Multiply values less than 0.1 by 100 and update column
cancer$AvgHouseholdSize[cancer$AvgHouseholdSize <= 0.1] <- cancer$AvgHouseholdSize[cancer$AvgHouseholdSize <= 0.1]*100
```

``` {r RemoveMissingValuesColumn, include = FALSE}
#removing PctEmployed16_Over and binnedInc from the dataset used for modelling.
cancer <- subset(cancer, select=-c(PctEmployed16_Over, binnedInc))
```
We add a variable describing the region in which observation the county is in. We split the US into 4 regions defined by the Census Bureau-designated regions and divisions [@Divisions]. These regions are Northeast, Midwest, South and West.

``` {r region variable, include=FALSE}
#Split Geography into county and state
cancer = separate(cancer,"Geography",c("County","State"),sep = ", ")
cancer <- cancer %>%
  mutate(Region = case_when(
  #Declaring the region that the states are in
    State %in% c("New Hampshire","New Jersey","New York","Maine","Massachusetts","Vermont","Connecticut","Pennsylvania","Rhode Island") ~ "Northeast",
    State %in% c("Wisconsin","Nebraska","Michigan","Minnesota","North Dakota","Missouri","Kansas","Ohio","Indiana","Iowa","Illinois","South Dakota") ~ "Midwest",
    State %in% c("West Virginia","Virginia","North Carolina","Alabama","Arkansas","Tennessee","Texas","Louisiana","Maryland","Mississippi","Kentucky","Delaware","District of Columbia","Florida","Oklahoma","South Carolina","Georgia") ~ "South",
    State %in% c("Washington","Nevada","New Mexico","California","Montana","Utah","Colorado","Wyoming","Oregon","Hawaii","Idaho","Alaska","Arizona") ~ "West"
  ))
  #Setting the Region variable as a factor
cancer$Region <-  factor(cancer$Region)
```
Lastly, we choose to ignore binnedInc in our modelling process as it represents the same force as medIncome.

## Modelling
``` {r redundancy, include = FALSE}
# Look at scatter plots and identify any relationships between deathRate.
par(mfrow = c(2,3))
with(cancer, scatter.smooth(deathRate~Edu18_24, ylab = "deathRate", xlab = "Edu18_24", lpars = list(col = "red", lwd = 2), main = "deathRate against Edu18_24", cex.main = 1))
with(cancer, scatter.smooth(deathRate~PctMarriedHouseholds, ylab = "deathRate", xlab = "PctMarriedHouseholds", lpars = list(col = "red", lwd = 2), main = "deathRate against PctMarriedHouseholds", cex.main = 1))
with(cancer, scatter.smooth(deathRate~PctBlack, ylab = "deathRate", xlab = "PctBlack", lpars = list(col = "red", lwd = 2), main = "deathRate against PctBlack", cex.main = 1))
with(cancer, scatter.smooth(deathRate~PctPrivateCoverage, ylab = "deathRate", xlab = "PctPrivateCoverage", lpars = list(col = "red", lwd = 2), main = "deathRate against PctPrivateCoverage", cex.main = 1))
with(cancer, scatter.smooth(deathRate~PctPublicCoverage, ylab = "deathRate", xlab = "PctPublicCoverage", lpars = list(col = "red", lwd = 2), main = "deathRate against PctPublicCoverage", cex.main = 1))
with(cancer, scatter.smooth(deathRate~povertyPercent, ylab = "deathRate", xlab = "povertyPercent", lpars = list(col = "red", lwd = 2), main = "deathRate against povertyPercent", cex.main = 1))
with(cancer, scatter.smooth(deathRate~medIncome, ylab = "deathRate", xlab = "medIncome", lpars = list(col = "red", lwd = 2), main = "deathRate against medIncome", cex.main = 1))
with(cancer, scatter.smooth(deathRate~incidenceRate, ylab = "deathRate", xlab = "incidenceRate", lpars = list(col = "red", lwd = 2), main = "deathRate against incidenceRate", cex.main = 1))
with(cancer, scatter.smooth(deathRate~Region, ylab = "deathRate", xlab = "Region", lpars = list(col = "red", lwd = 2), main = "deathRate against Region", cex.main = 1))
with(cancer, scatter.smooth(deathRate~MedianAgeMale, ylab = "deathRate", xlab = "MedianAgeMale", lpars = list(col = "red", lwd = 2), main = "deathRate against MedianAgeMale", cex.main = 1))
with(cancer, scatter.smooth(deathRate~MedianAgeFemale, ylab = "deathRate", xlab = "MedianAgeFemale", lpars = list(col = "red", lwd = 2), main = "deathRate against MedianAgeFemale", cex.main = 1))
with(cancer, scatter.smooth(deathRate~AvgHouseholdSize, ylab = "deathRate", xlab = "AvgHouseholdSize", lpars = list(col = "red", lwd = 2), main = "deathRate against AvgHouseholdSize", cex.main = 1))
with(cancer, scatter.smooth(deathRate~PercentMarried, ylab = "deathRate", xlab = "PercentMarried", lpars = list(col = "red", lwd = 2), main = "deathRate against PercentMarried", cex.main = 1))
with(cancer, scatter.smooth(deathRate~PctEmpPrivCoverage, ylab = "deathRate", xlab = "PctEmpPrivCoverage", lpars = list(col = "red", lwd = 2), main = "deathRate against PctEmpPrivCoverage", cex.main = 1))
with(cancer, scatter.smooth(deathRate~PctUnemployed16_Over, ylab = "deathRate", xlab = "PctUnemployed16_Over", lpars = list(col = "red", lwd = 2), main = "deathRate against PctUnemployed16_Over", cex.main = 1))
```
The variables we will include in the initial model are those that show a relationship with deathRate. From our initial EDA we can observe that most variables show some type of relationship with deathRate. However, it is not clear to see whether any do not. For now, we will assume all predictor variables do and keep them in the model, especially as we have not yet tested for multicollinearity which could affect the model. We will soon do an analysis of variance to check the significance of the relationships between predictors and deathRate.

``` {r adequacy, include = FALSE}
# Creating a linear model with all variables from the current dataset
Model0 <- lm(deathRate ~ Region + incidenceRate + medIncome + povertyPercent + MedianAgeMale + MedianAgeFemale + AvgHouseholdSize + PercentMarried + PctBlack + PctMarriedHouseholds + PctPrivateCoverage + PctPublicCoverage + PctEmpPrivCoverage + PctUnemployed16_Over + Edu18_24, data = cancer)
```
We will check the adequacy of our initial model by looking at the residuals vs fitted and scale location plots to see if the model assumptions are fulfilled:


``` {r, echo = FALSE, eval = TRUE,fig.height=1.8,fig.width=6, fig.cap = "Checking for linearity and homoscedasticity of the initial model"}
# Identify any heteroscedasticity and non-linearity
par(mar=c(1.8, 4.4, 1.8, 1.9))
par(mfrow=c(1,2))
plot(Model0, 1, cex=0.5)
plot(Model0, 3, cex=0.5)
```

We can see from the plots that there exists heteroscedasticity and non-linearity. We know that some predictor variables are violating model assumptions, so we will need to perform some transformations of some variables to fix these. We do this by using Box-Cox and observations after trialling log transformations. We first transform the predictor variables to satisfy homoscedascity, these include the following transformations: $log$(PctBlack$+0.1$), incidenceRate^1.5^, PctUnemployed16_Over^0.5^ and  $log$(medIncome).

```{r, include=FALSE}
#Box cox Power transformation UDF
boxcox <- function(x, p){ 
  if (p == 0){
    return(log(x)) 
  }
  else{
    return((x^p - 1)/p)
  } 
}
```

``` {r transformations, include = FALSE}
# Declare new dataframe from old
cancer5=cancer
#Scaling PctBlack 
cancer5$PctBlack=(cancer5$PctBlack) + 0.1
# Transformations
cancer5$incidenceRate=boxcox(cancer5$incidenceRate, p = 1.5)
cancer5$medIncome=log(cancer5$medIncome)
cancer5$PctUnemployed16_Over=boxcox(cancer5$PctUnemployed16_Over, p = 0.5)
cancer5$PctBlack=log(cancer5$PctBlack)

```

``` {r checkingassumptions print, include = FALSE, eval = TRUE}
#Check for any updated heteroscedascity and linearity
plot(lm(deathRate ~ incidenceRate, data = cancer5), 1)
plot(lm(deathRate ~ incidenceRate, data = cancer5), 3)
plot(lm(deathRate ~ medIncome, data = cancer5), 1)
plot(lm(deathRate ~ medIncome, data = cancer5), 3)
plot(lm(deathRate ~ PctBlack, data = cancer5), 1, cex=0.5)
plot(lm(deathRate ~ PctBlack, data = cancer5), 3, cex=0.5)
```


``` {r checkingassumptions, echo = FALSE, eval = TRUE,out.height='80%',out.width='80%', fig.cap = "Showing the improved linearity and homoscedasticity of PctUnemployed16Over"}
par(mar=c(1.8, 4.4, 2, 1.8))
par(mfrow=c(1,2))
plot(lm(deathRate ~ PctUnemployed16_Over, data = cancer5), 1, cex=0.5)
plot(lm(deathRate ~ PctUnemployed16_Over, data = cancer5), 3, cex=0.5)
```

These transformations show evidence of homoscedascity and linearity thus we will update the new model with the transformed variables. The three transformed variables that are not shown here are in the appendix. After transforming the varibales, we check the model assumptions for this updated model and notice the model assumptions are being violated, this could be due to influencial outliers - so we will check this.

``` {r adequacy2, echo = FALSE, eval = TRUE, fig.height=1.8,fig.width=6, fig.cap = "The updated model shows signs of linearity and reduced heteroscedasticity"}
# Creating updated model with transformed variables
par(mar=c(1.8, 4.4, 1.8, 1.9))
Model1 <- lm(deathRate ~ Region + incidenceRate + medIncome + povertyPercent + MedianAgeMale + MedianAgeFemale + AvgHouseholdSize + PercentMarried + PctBlack + PctMarriedHouseholds + PctPrivateCoverage + PctPublicCoverage + PctEmpPrivCoverage + PctUnemployed16_Over + Edu18_24, data = cancer5)

# Identify any heteroscedascity and non-linearity 
par(mfrow=c(1,2))
plot(Model1, 1, cex=0.5)
plot(Model1, 3,cex=0.5)
```

## Outliers  

If outliers have high influence then they will need to be considered for removal. Influencial outliers are those that effect the slope of the model. We will calculate if any influencial outliers exist by using residual leverage plots and DFBETAS. Any DFBETA values greater than 0.4 and also outliers that have high leverage will be removed from the data.


``` {r CheckOutliers, include = FALSE}
par(mfrow=c(1,2))
ols_plot_resid_lev(Model1)
# This plot shows that the observations that are outliers with high leverage that we declare highly influencial are 2682, 282, 166, 2727, and 2714.
par(mfrow=c(2,2))
ols_plot_dfbetas(Model1)
# This plot shows that the observations 2682, 282, 166, 2727, and 2714 all have a DFBETA > 0.4.
```

We declare the following observations to be influencial and we will remove these from our dataset: 2682, 282, 166, 2727, 1490 and 2714. We investigated the nature of these observation's influence further, our research reinforced our decision to remove as for example Union County, Florida is a county where prisoners with lung cancer are sent (@Outliers).

``` {r removeoutliers, include = FALSE}
#remove the influencial outliers.
library(dplyr)
cancer5 <-  slice(cancer5, -c(2682, 282, 166, 2727, 2714, 1490))
```

``` {r Model3, include = FALSE}
Model3 <- lm(deathRate ~ Region + incidenceRate + medIncome + povertyPercent + MedianAgeMale + MedianAgeFemale + AvgHouseholdSize + PercentMarried + PctBlack + PctMarriedHouseholds + PctPrivateCoverage + PctPublicCoverage + PctEmpPrivCoverage + PctUnemployed16_Over + Edu18_24, data = cancer5)
par(mfrow=c(2,2))
plot(Model3)
```

``` {r checklargesample, include = FALSE}
summary(cancer5)
#Shows 3041 observations 
```
After removing these influential observations we have 3041 observations in our sample space, this is a large sample space and so we can pay less attention to the errors being normally distributed as it is not a major concern due to the likelihood of errors being large. 

We check if deathRate highly correlates to any of the explanatory variables by analysing the model's summary statistics and test the null hypothesis that there is no linear association between deathRate and all individual predictor variables. 

``` {r summarymod3, include = FALSE}
anova(Model3)
```

We focus on the relationships between deathRate and medianAgeMale, deathRate and medianAgeFemale, deathRate and PercentMarried, and deathRate and PctBlack, and deathRate and PctMarriedHouseholds. We observe the p-values for these which, respectively, show 0.656,  0.936, 0.360, and 0.141. As the p-value is above, 0.05 we fail to reject the null hypothesis for these relationships. All other predictor variables reject the null hypothesis. We do not yet remove any variables from the model as we first want to check any existing redundancies.

So, we can determine that our response variable has a relationship with at least one of the variables in the model and we can further test which model is optimal by testing for redundancy. We will spot this by comparing the summary output with the scatter plots we saw in our EDA. 

The scatter plots show a negative correlation between deathRate and PctMarriedHouseholds and PctEmpPrivCoverage, which are not negative in the summary of the model. Moreover, the estimate is negative for PovertyPercent and PctBlack in the summary statistics but clearly not in the plot. This shows there exists redundancy and can be explained potentially by any collinearity. The other redundancy we see is that MedianAgeMale and MedianAgeFemale plot against deathRate does not correlate with the estimate in the summary statistic. These redundancies could be caused by multicollinearity.

``` {r multicolliearity2, include = FALSE}
#Showing the coefficients of the predictors and comparing with scatter plots
summary(Model3)
# Look at scatter plots and compare weak positive correlations to the correlations shown in original plots.
par(mfrow = c(2,3))
with(cancer5, scatter.smooth(deathRate~Edu18_24, ylab = "deathRate", xlab = "Edu18_24", lpars = list(col = "red", lwd = 2), main = "deathRate against Edu18_24", cex.main = 1))
with(cancer5, scatter.smooth(deathRate~PctMarriedHouseholds, ylab = "deathRate", xlab = "PctMarriedHouseholds", lpars = list(col = "red", lwd = 2), main = "deathRate against PctMarriedHouseholds", cex.main = 1))
with(cancer5, scatter.smooth(deathRate~PctBlack, ylab = "deathRate", xlab = "PctBlack", lpars = list(col = "red", lwd = 2), main = "deathRate against PctBlack", cex.main = 1))
with(cancer5, scatter.smooth(deathRate~PctPrivateCoverage, ylab = "deathRate", xlab = "PctPrivateCoverage", lpars = list(col = "red", lwd = 2), main = "deathRate against PctPrivateCoverage", cex.main = 1))
with(cancer5, scatter.smooth(deathRate~PctPublicCoverage, ylab = "deathRate", xlab = "PctPublicCoverage", lpars = list(col = "red", lwd = 2), main = "deathRate against PctPublicCoverage", cex.main = 1))
with(cancer5, scatter.smooth(deathRate~povertyPercent, ylab = "deathRate", xlab = "povertyPercent", lpars = list(col = "red", lwd = 2), main = "deathRate against povertyPercent", cex.main = 1))
with(cancer5, scatter.smooth(deathRate~medIncome, ylab = "deathRate", xlab = "medIncome", lpars = list(col = "red", lwd = 2), main = "deathRate against medIncome", cex.main = 1))
with(cancer5, scatter.smooth(deathRate~incidenceRate, ylab = "deathRate", xlab = "incidenceRate", lpars = list(col = "red", lwd = 2), main = "deathRate against incidenceRate", cex.main = 1))
with(cancer5, scatter.smooth(deathRate~Region, ylab = "deathRate", xlab = "Region", lpars = list(col = "red", lwd = 2), main = "deathRate against Region", cex.main = 1))
with(cancer5, scatter.smooth(deathRate~MedianAgeMale, ylab = "deathRate", xlab = "MedianAgeMale", lpars = list(col = "red", lwd = 2), main = "deathRate against MedianAgeMale", cex.main = 1))
with(cancer5, scatter.smooth(deathRate~MedianAgeFemale, ylab = "deathRate", xlab = "MedianAgeFemale", lpars = list(col = "red", lwd = 2), main = "deathRate against MedianAgeFemale", cex.main = 1))
with(cancer5, scatter.smooth(deathRate~AvgHouseholdSize, ylab = "deathRate", xlab = "AvgHouseholdSize", lpars = list(col = "red", lwd = 2), main = "deathRate against AvgHouseholdSize", cex.main = 1))
with(cancer5, scatter.smooth(deathRate~PercentMarried, ylab = "deathRate", xlab = "PercentMarried", lpars = list(col = "red", lwd = 2), main = "deathRate against PercentMarried", cex.main = 1))
with(cancer5, scatter.smooth(deathRate~PctEmpPrivCoverage, ylab = "deathRate", xlab = "PctEmpPrivCoverage", lpars = list(col = "red", lwd = 2), main = "deathRate against PctEmpPrivCoverage", cex.main = 1))
with(cancer5, scatter.smooth(deathRate~PctUnemployed16_Over, ylab = "deathRate", xlab = "PctUnemployed16_Over", lpars = list(col = "red", lwd = 2), main = "deathRate against PctUnemployed16_Over", cex.main = 1))
```

## Multicollinearity
``` {r multicollinearity, include = FALSE}
library(olsrr)
ols_coll_diag(Model3)
#Look for low tolerance, VIF > 5 and condition index > 30
```
This shows that MedianAgeMale, MedianAgeFemale, PctPublicCoverage, PctMarriedHouseholds, PctPrivateCoverage, PercentMarried, povertyPercent and medIncome all had VIF >5, which indicates multicollinearity exists in the model. When looking at the condition index's (CI) above 30, the variance decomposition proportion (v.c.p) above 0.3, which we used as the cut off, establishes those variables which provide evidence of, to some degree, multicollinearity [@ConditionIndex]. We notice eight CI's above 31 and six of these, with CI's equalling 679.76, 174.74, 124.7, 101.87, 80.46, 25.92, have some variables with v.c.p above 0.3. These are (from most significant CI to least): povertyPercent and medIncome; PctMarriedHouseholds, PercentMarried & AvgHouseholdSize; MedianAgeMale and MedianAgeFemale; PctMarriedHouseholds and PercentMarried; PctPrivateCoverage and Edu18_24; Edu18_24 and PctEmpPrivCoverage. When the v.c.p is bigger for a larger CI, this indicates the multicollinearity is more significant.

```{r, include=FALSE}
#defining boxcox function
boxcox <- function(x, p){ 
  if (p == 0){
    return(log(x)) 
  }
  else{
    return((x^p - 1)/p)
  } 
}
```

## Stepwise Regression

**Initial Ideas**

Before attempting to fit any models or try stepwise regression, we looked at what variables we thought would be most and least important in determining the death rate within a county. In the EDA we saw `PctEmpPrivCoverage` was strongly correlated to `PctPrivateCoverage` so we excluded `PctEmpPrivCoverage`. in addition, the median age variables and `AvgHouseholdSize` had no relationship to death rate and had very high p-values in the full model so these were excluded from our initial stepwise full model.

**AIC Stepwise Regression**

We first attempted to use AIC but this returned a model with 9 variables in it as well as an intercept term. Despite the fact this model is a good explanation of the data, it is far from simple and would be more difficult to interpret properly. As a result we opted to go forward using BIC as this gave a heavier penalty for having more variables.

```{r Define null and full models, include=FALSE}
fullTransModel=lm(deathRate ~ Edu18_24 + medIncome + PctMarriedHouseholds + 
                    PctBlack + PctPublicCoverage + PctPrivateCoverage + 
                    PctUnemployed16_Over + PercentMarried + povertyPercent + 
                    incidenceRate + Region,
                  data = cancer5)
nullTransModel=lm(deathRate~1,data=cancer5)
```


```{r AIC Stepwise, include=FALSE}
AICmodel1 = step(nullTransModel,direction='both',scope=formula(fullTransModel))
ols_coll_diag(AICmodel1)
```

**BIC Stepwise Regression**

```{r BIC Stepwise 1, include=FALSE}
BICmodel1 = step(nullTransModel,direction='both',scope=formula(fullTransModel), k=log(3041))
ols_coll_diag(BICmodel1)
summary(BICmodel1)
```

Our first model for the BIC stepwise regression model had 6 variables in it which is a much easier to interpret than what the AIC suggested. However, the condition number was 265. This means that two of the rows in the condition matrix are near to liner combinations of each other and so we have an issue with collinearity. The output suggested that it was the median income and the intercept term. We initially thought that having an intercept term may not be the best approach going forward and so we removed it at this point.

```{r BIC Stepwise 2, include=FALSE}
fullTransModel=lm(deathRate ~ 0 + Edu18_24 + medIncome + PctMarriedHouseholds + 
                    PctBlack + PctPublicCoverage + PctPrivateCoverage + 
                    PctUnemployed16_Over + PercentMarried + povertyPercent + 
                    incidenceRate + Region,
                  data = cancer5)
nullTransModel=lm(deathRate~0,data=cancer5)

BICmodel2 = step(nullTransModel,direction='both',scope=formula(fullTransModel), k=log(3041))
ols_coll_diag(BICmodel2)
summary(BICmodel2)
```

Despite removing the intercept term, there were still collinearity issues between the median income, education coefficient and regions. Median income appeared to be a common theme and there were alternative measures of wealth available. As a result we removed median income from our full model and looked at if the collinearity was solved then.

```{r BIC Stepwise 3, include=FALSE}
fullTransModel=lm(deathRate ~ 0 + Edu18_24 + PctMarriedHouseholds + 
                    PctBlack + PctPublicCoverage + PctPrivateCoverage + 
                    PctUnemployed16_Over + PercentMarried + povertyPercent + 
                    incidenceRate + Region,
                  data = cancer5)

BICmodel3 = step(nullTransModel,direction='both',scope=formula(fullTransModel), k=log(3041))
ols_coll_diag(BICmodel3)
summary(BICmodel3)
```

The condition number decreased but was still 65, which is too large. At this point the problematic variables were the regions, percentage of married households and the education coefficient. We decided that we would remove the regions since if a region has a higher death rate then this is usually captured by a higher incidence rate. As a result, we removed the regions from our full model and repeated the stepwise regression again.

```{r BIC Stepwise 4, include=FALSE}
fullTransModel=lm(deathRate ~ 0 + Edu18_24 + PctMarriedHouseholds + 
                    PctBlack + PctPublicCoverage + PctPrivateCoverage + 
                    PctUnemployed16_Over + PercentMarried + povertyPercent + 
                    incidenceRate,
                  data = cancer5)

BICmodel4 = step(nullTransModel,direction='both',scope=formula(fullTransModel), k=log(3041))
ols_coll_diag(BICmodel4)
summary(BICmodel4)
```

After analysing the condition numbers and the VIFs from this model we were able to conclude that collinearity was no longer an issue. The condition number was below 19 and the VIFs were all below 3.5. Consequently, we decided that we could use this model as the model selected by stepwise regression.

**Check Model Assumptions**

The graphs below show that the Residuals vs Fitted plot and the Scale-Location plot look very slightly quadratic which is shown by the red line. However, the residuals appear to be randomly distributed around 0 and there seems to be no correlation between them. There also does not appear to be issues with heteroscedsticity. The errors in the Normal Q-Q plot appear to fall away from the line in the tails but we have 3041 data points and so the normality in errors does not need to be followed as strictly. Finally the Cook's Distance plot shows there are some points which had some influence but there was no justification for omitting them as an outlier.

```{r Check Assumptions, echo=FALSE,out.height='70%',out.width='70%', fig.cap = "Checking the model assumptions of the BICModel4"}
par(mar=c(1.8, 4.4, 2, 1.8))
par(mfrow=c(1,2))
  plot(BICmodel4,1, cex=0.5)
  plot(BICmodel4,3, cex=0.5)
}
```

## LASSO
\normalsize

```{r, include = FALSE}
cancerLASSO <- cancer5[,-c(1,2,17)]
cancerLASSO <- model.matrix(~., data = cancerLASSO)
LASSO <- glmnet(cancerLASSO, cancer5$deathRate, alpha = 1) 
set.seed(6)
cvLASSO <- cv.glmnet(cancerLASSO, cancer5$deathRate, alpha = 1)
LASSOmodel <- glmnet(cancerLASSO, cancer5$deathRate, alpha = 1, lambda = cvLASSO$lambda.1se)
LASSOpredicted <- predict(LASSOmodel, newx = cancerLASSO)
LASSOresiduals <- cancer5$deathRate - LASSOpredicted
LASSO_Rsquared <- 1 - (sum(LASSOresiduals^2)/sum((cancer5$deathRate - mean(cancer5$deathRate))^2))
```

Using the transformations discussed we first ran an LAR algorithm to obtain coefficients of our variables for different values of the penalty parameter, lambda. Figure ? shows a plot of these values against log(lambda).

```{r Cross Validation LASSO, echo = FALSE,out.height='50%',out.width='50%', fig.cap = "Needs to be done"} 
par(mfrow = c(1,1))
plot(LASSO, "lambda", label = T)
abline(v = log(cvLASSO$lambda.min), col = "blue")
abline(v = log(cvLASSO$lambda.1se), col = "red")
legend("bottomleft",legend=c("Minimum lambda", "1 standard error larger lambda"),lty=c(1,1),col=c("blue","red"), ins=0.05, bty = "n")
```
We also performed cross validation to see how the mean squared error changes with lambda. Both the minimising lambda and the lambda one standard error away are shown in figure ?. We chose to use the value of lambda one standard error away since using the minimising lambda leads to no parameters being shrunk to zero. A model including every parameter is undesirable so we instead build our model using the value of lambda one standard error above the minimum.  The model we get from this has the coefficients as seen in the findings section.  

**Model Diagnostics**
The residuals vs fitted plot suggests our model is nicely linear and the scale-location plot suggests homoscedastic residuals.  
```{r, echo = FALSE, fig.width=9 , fig.height=2.25, fig.cap = "Needs to be done"}
par(mfrow = c(1,2),mar=c(1.8, 4.4, 1.8, 1.9))
bin <- scale(LASSOresiduals)
bin1 <- which(bin < 0)
bin[bin1] <- -bin[bin1]
plot(LASSOpredicted, sqrt(bin), main = "Scale Location", xlab = "Fitted values", ylab = "Square Root of Standardised Residuals",cex=0.5)
plot(LASSOpredicted, LASSOresiduals, main = "Residuals vs Fitted plot for LASSO model", xlab = "Fitted values", ylab = "Residuals", cex=0.5)
```
This model also returns an R squared value of 0.4849. This model also had a mean squared error of 389.4. These all suggest a strong predictive power. Furthermore all the variables, with the exception of median income, which we included in our original model remained. This suggests this model has a strong explanatory power. 
We also computed the variance inflation factors which returned a VIF no higher than 2 for each variable suggesting no issues with multicollinearity.



## Ridge Regression
Due to multicollinearity existing in the model we will perform a ridge regression, this will not contribute to any variable selection but will instead improve the predictive power of the model by shrinking parameter estimates towards 0. 

``` {r ridge, include = FALSE}
set.seed(3041)
ridge.fit <- cv.glmnet(model.matrix(Model3), cancer5$deathRate, alpha = 0, nrow = 3041)

optimallamda.ridge <- ridge$lambda.min
predicteddeathRate.ridge <- predict(ridge.fit, s = optimallamda, newx = model.matrix(Model3))

predicted.ridge <- cv.glmnet(model.matrix(Model3), cancer5$deathRate, type.measure = 'mse', keep = TRUE, alpha = 0, nrow = 3041)
lamda.ridge <- predicted.ridge$lambda.min
mse2.ridge <- predicted.ridge$cvm[predicted.ridge$lambda == lamda.ridge]
mse2.ridge

rsquared1.ridge <- sum((cancer5$deathRate - mean(cancer5$deathRate))^2)
rsquared2.ridge <- sum((predicteddeathRate.ridge - cancer5$deathRate)^2)

rsquared.ridge <- (1 - (rsquared2/rsquared1))
rsquared.ridge
```

The Ridge regression produces a linear model with an R^2 value of 0.484, and a mean square error of 378.5, which will be a reduced value as the ridge regression has improved variance at the cost of increased bias in the model. The model still contains all predictor variables and so would be good to improve the predictive power of the model, but considering all variables are still included, an R^2 value of 4.84 is not great, and it will be better to use another penalized likelihood method such as LASSO.

## Comparison of models found from variable selection

To compare how well the models predict values we used leave one out cross validation (LOOCV). We ran this on the model suggested by BIC stepwise regression and the LASSO model and the summary is given in the table below.
\tiny
```{r Cross validation, echo=FALSE}
crossVal=as.table(matrix(data=c(20.98,16.09,0.4176,19.73,NA,0.4849),nrow=2,ncol=3,byrow = TRUE))
rownames(crossVal)=c('BIC stepwise model','LASSO Model')
colnames(crossVal)=c('Root Mean Squared Error','Mean Absolute Error','R-Squared')
knitr::kable(crossVal, caption = "Errors and goodness of fit measures for BIC stepwise regression model")
```
\normalsize
The table shows that the LASSO model has better predictive and explanatory power through its R^2^ value and its mean squared error.

## Major determinants of high mortality rates

The major determinants we calculated by taking the range of the values that a variable can take and multiplying it by its coefficient. This gives the amount `deathRate` can fluctuate as a result of that particular variable. For example for incidence rate the amount it could affect `deathRate` was $(12849.580-2044.087)*0.01=109.04$. This was repeated for each variable. `incidenceRate` (109.04) was the highest followed by `medIncome` (40.42), then `PctUnemployed16_Over` (20.19), `Region` (15.4), `Edu18_24` (14.96), `PctPrivateCoverage` (14.85), `PctPublicCoverage` (6.77).

\pagebreak

# \underline{References}

<div id="refs"></div>

\pagebreak

# Appendix
\tiny
## Cleaning the Data
```{r, eval=FALSE}
#Correlation of employment variables
cancerNoNAs <- na.omit(cancerRaw)
cor(cancerNoNAs$PctUnemployed16_Over,cancerNoNAs$PctEmployed16_Over)
cancer$AvgHouseholdSize[cancer$AvgHouseholdSize <= 0.1] <- cancer$AvgHouseholdSize[cancer$AvgHouseholdSize <= 0.1]*100
#removing PctEmployed16_Over and binnedInc from the dataset used for modelling.
cancer <- subset(cancer, select=-c(PctEmployed16_Over, binnedInc))
#Creating a Region variable, as a factor
cancer = separate(cancer,"Geography",c("County","State"),sep = ", ")
cancer <- cancer %>%
  mutate(Region = case_when(
    State %in% c("New Hampshire","New Jersey","New York","Maine",
                 "Massachusetts","Vermont","Connecticut","Pennsylvania",
                 "Rhode Island") ~ "Northeast",
    State %in% c("Wisconsin","Nebraska","Michigan","Minnesota",
                 "North Dakota","Missouri","Kansas","Ohio",
                 "Indiana","Iowa","Illinois","South Dakota") ~ "Midwest",
    State %in% c("West Virginia","Virginia","North Carolina","Alabama",
                 "Arkansas","Tennessee","Texas","Louisiana",
                 "Maryland","Mississippi","Kentucky","Delaware",
                 "District of Columbia","Florida","Oklahoma","South Carolina",
                 "Georgia") ~ "South",
    State %in% c("Washington","Nevada","New Mexico","California",
                 "Montana","Utah","Colorado","Wyoming",
                 "Oregon","Hawaii","Idaho","Alaska","Arizona") ~ "West"
  ))
cancer$Region <-  factor(cancer$Region)
```

## Initial Model Analysis
``` {r redundancyAppendix, include = FALSE}
# Looking at scatter plots and identifying any relationships between deathRate and the predictor variables.
par(mfrow = c(2,3))
with(cancer, scatter.smooth(deathRate~Edu18_24, ylab = "deathRate", xlab = "Edu18_24", lpars = list(col = "red", lwd = 2), main = "deathRate against Edu18_24", cex.main = 1))
with(cancer, scatter.smooth(deathRate~PctMarriedHouseholds, ylab = "deathRate", xlab = "PctMarriedHouseholds", lpars = list(col = "red", lwd = 2), main = "deathRate against PctMarriedHouseholds", cex.main = 1))
# The same code was used for all variables in the dataset.

```
``` {r adequacyAppendix, eval = FALSE}
# Creating a linear model with all variables from the current dataset
Model0 <- lm(deathRate ~ Region + incidenceRate + medIncome + povertyPercent + MedianAgeMale + MedianAgeFemale + AvgHouseholdSize + PercentMarried + PctBlack + PctMarriedHouseholds + PctPrivateCoverage + PctPublicCoverage + PctEmpPrivCoverage + PctUnemployed16_Over + Edu18_24, data = cancer)
#We updated this model after transformations and named it Model1 with the data = cancer5
# We again updated this model after removing influential observations and named it Model3.

# Checking the output of the model to see significance
summary(Model0)

# Identify any heteroscedasticity and non-linearity in the initial model using residual vs fitted and scale location plots
par(mar=c(1.2, 4.4, 1.2, 1.9))
par(mfrow=c(1,2))
plot(Model0, 1, cex=0.5)
plot(Model0, 3, cex=0.5)
# We repeated this for Model1 to check if homoscedasticity and linearity have improved after transformations
# We repeated this for Model3 to once again check the model assumptions.

```
## Updated Model with Transformed Predictors
```{r, eval=FALSE}
#Box cox Power transformation UDF - used to find power transformations
boxcox <- function(x, p){ 
  if (p == 0){
    return(log(x)) 
  }
  else{
    return((x^p - 1)/p)
  } 
}

# Declaring new dataframe from old
cancer5=cancer
#Scaling PctBlack to remove the issue of 0 values before log
cancer5$PctBlack=cancer5$PctBlack + 0.1
# Transformations using boxcox to fix model violations
cancer5$incidenceRate=boxcox(cancer5$incidenceRate, p = 1.5)
cancer5$medIncome=log(cancer5$medIncome)
cancer5$PctUnemployed16_Over=boxcox(cancer5$PctUnemployed16_Over, p = 0.5)
cancer5$PctBlack=log(cancer5$PctBlack)

#Check for any updated heteroscedascity and linearity in incidence rate and medIncome (not shown)
plot(lm(deathRate ~ incidenceRate, data = cancer5), 1)
plot(lm(deathRate ~ incidenceRate, data = cancer5), 3)
plot(lm(deathRate ~ medIncome, data = cancer5), 1)
plot(lm(deathRate ~ medIncome, data = cancer5), 3)
# The same code was used to check for PctBlack and PctUnemployed16_Over as shown in Statistical Methodology.
```

## Outliers/Influencial Obsevations
``` {r CheckOutliersAppendix, eval = FALSE}
par(mfrow=c(1,2))
ols_plot_resid_lev(Model1)
# This plot shows that the observations that are outliers with high leverage
par(mfrow=c(2,2))
ols_plot_dfbetas(Model1)
# This plot shows the observations DFBETA which we have used to produce the table in the Outliers section. These are all outliers with DFBETA > 0.4

#remove the influencial outliers in the dataset
library(dplyr)
cancer5 <-  slice(cancer5, -c(2682, 282, 166, 2727, 2714, 1490))
#Check the number of observations in the sample space
summary(cancer5)
```

## Check Redundancy of the Linear Model
``` {r summarymod3Appendix, eval = FALSE}
# Use the summary of the current model and compare to scatter plots that show correlations to check for significance between predictors in the model with deathRate
summary(Model3)

# Look at scatter plots and compare weak positive correlations to the correlations shown in original plots.
par(mfrow = c(2,3))
with(cancer5, scatter.smooth(deathRate~Edu18_24, ylab = "deathRate", xlab = "Edu18_24", lpars = list(col = "red", lwd = 2), main = "deathRate against Edu18_24", cex.main = 1))
with(cancer5, scatter.smooth(deathRate~PctMarriedHouseholds, ylab = "deathRate", xlab = "PctMarriedHouseholds", lpars = list(col = "red", lwd = 2), main = "deathRate against PctMarriedHouseholds", cex.main = 1))
# The same code was used for all predictor variables in Model 3

```

## Multicollinearity
``` {r multicollinearityAppendix, eval = FALSE}
library(olsrr)
# Produce tables showing the VIF of variables in the current model, as well as tolerance, condition index and variance decomposition proportion.
ols_coll_diag(Model3)
#Look for low tolerance, VIF > 5 and condition index > 30 to see any issues with multicollinearity.
```


## Variable Selection: Stepwise Regression 
```{r Define null and full modelsAppendix, echo=FALSE}
# Create the model including all variables after removal of the variables showing no significance of a relationship with deathRate
fullTransModel=lm(deathRate ~ Edu18_24 + medIncome + PctMarriedHouseholds + 
                    PctBlack + PctPublicCoverage + PctPrivateCoverage + 
                    PctUnemployed16_Over + PercentMarried + povertyPercent + 
                    incidenceRate + Region,
                  data = cancer5)
# Create a model including no variables
nullTransModel=lm(deathRate~1,data=cancer5)
```

### AIC Stepwise
```{r AIC StepwiseAppendix, eval=FALSE}
#Created a stepwise model using AIC
AICmodel1 = step(nullTransModel,direction='both',scope=formula(fullTransModel))
ols_coll_diag(AICmodel1)
```

### BIC Stepwise

```{r BIC Stepwise 1Appendix, eval=FALSE}
#Created a stepwise model using BIC
BICmodel1 = step(nullTransModel,direction='both',scope=formula(fullTransModel), k=log(3041))
ols_coll_diag(BICmodel1)
summary(BICmodel1)
# We did the same for BICmodel2 after adding an intercept term of 0 to the fullTransmodel and nullTransmodel.
# We did the same for BICmodel3 after removing medIncome. 
# We did the same for BICmodel4 after removing medIncome and Region. 

#For BIC Stepwise 2
fullTransModel=lm(deathRate ~ 0 + Edu18_24 + medIncome + PctMarriedHouseholds + 
                    PctBlack + PctPublicCoverage + PctPrivateCoverage + 
                    PctUnemployed16_Over + PercentMarried + povertyPercent + 
                    incidenceRate + Region,
                  data = cancer5)
# Same code but removed medIncome for the BICmodel3 
# Same code but removed medIncome and Region for the BICmodel4 
nullTransModel=lm(deathRate~0,data=cancer5)
```

```{r Check AssumptionsAppendix, eval=FALSE}
# Checking the model assumptions of BICmodel4
par(mar=c(1.5, 4.4, 2, 1.5))
par(mfrow=c(2,2))
for (i in (1:4)) {
  plot(BICmodel4,i, cex=0.5)
}
```

### Cross Validation of BIC Stepwise Model
```{r Cross validationAppendix, eval=FALSE}
#Included in Statistical methodology
crossVal=as.table(matrix(data=c(20.98,16.09,0.4176),nrow=1,ncol=3,byrow = TRUE))
rownames(crossVal)=c('BIC stepwise model')
colnames(crossVal)=c('Root Mean Squared Error','Mean Absolute Error','R-Squared')
knitr::kable(crossVal, caption = "Errors and goodness of fit measures for BIC stepwise regression model")
```

## LASSO Regression
```{r, eval = FALSE}
cancerLASSO <- cancer5[,-c(1,2,17)]
cancerLASSO <- model.matrix(~., data = cancerLASSO)
LASSO <- glmnet(cancerLASSO, cancer5$deathRate, alpha = 1) 
set.seed(6)
cvLASSO <- cv.glmnet(cancerLASSO, cancer5$deathRate, alpha = 1)
LASSOmodel <- glmnet(cancerLASSO, cancer5$deathRate, alpha = 1, lambda = cvLASSO$lambda.1se)
LASSOpredicted <- predict(LASSOmodel, newx = cancerLASSO)
LASSOresiduals <- cancer5$deathRate - LASSOpredicted
LASSO_Rsquared <- 1 - (sum(LASSOresiduals^2)/sum((cancer5$deathRate - mean(cancer5$deathRate))^2))
#This block of code is everything needed to reproduce the LASSO coefficents found. It also defined the residuals, fitted values and R squared value for the 
#corresponding model since LASSO just selects coefficents so such thing need to be done manually
```


```{r Cross Validation LASSOAppendix, eval = FALSE} 
par(mfrow = c(1,1))
plot(LASSO, "lambda", label = T)
abline(v = log(cvLASSO$lambda.min), col = "blue")
abline(v = log(cvLASSO$lambda.1se), col = "red")
legend("bottomleft",legend=c("Minimum lambda", "1 standard error larger lambda"),lty=c(1,1),col=c("blue","red"), ins=0.05, bty = "n")
#This plots log(lambda) against the coefficents for each variable calculated from LASSO regression
```

```{r, eval = FALSE}
par(mfrow = c(1,2),mar=c(2, 4.4, 2, 1.9))
bin <- scale(LASSOresiduals)
bin1 <- which(bin < 0)
bin[bin1] <- -bin[bin1]
plot(LASSOpredicted, sqrt(bin), main = "Scale Location", xlab = "Fitted values", ylab = "Square Root of Standardised Residuals",cex=0.5)
plot(LASSOpredicted, LASSOresiduals, main = "Residuals vs Fitted plot for LASSO model", xlab = "Fitted values", ylab = "Residuals", cex=0.5)
#This gives the plots for residuals vs fitted and scale location plot for the model generated using LASSO coefficents.
```

```{r, eval = FALSE}
LASSOvif <- function(x){
cancervif <- cancer5[,-c(1,2,17)]
cancervif <- cancervif[,c("incidenceRate","medIncome","PctUnemployed16_Over","PctPrivateCoverage","PctPublicCoverage","Edu18_24","Region")]
cancervif <- cancervif[,-x]
cancervif <- model.matrix(~., data = cancervif)
set.seed(105)
cvLASSOvif <- cv.glmnet(cancervif, cancer5$deathRate, alpha = 1)
LASSOmodelvif <- glmnet(cancervif, cancer5$deathRate, alpha = 1, lambda = cvLASSOvif$lambda.1se)
LASSOresidualsvif <- cancer5$deathRate - predict(LASSOmodelvif, newx = cancervif)
LASSO_Rsquaredvif <- 1 - (sum(LASSOresidualsvif^2)/sum((cancer5$deathRate - mean(cancer5$deathRate))^2))
return(1/(1-LASSO_Rsquaredvif))
}
#The idea of this UDF is to find the VIF factor for the LASSO model. the input refers to the index of the coefficent being looked at.
```


## Ridge Regression

``` {r ridgeAppendix, eval = FALSE}
set.seed(3041)
ridge.fit <- cv.glmnet(model.matrix(Model3), cancer5$deathRate, alpha = 0, nrow = 3041)

optimallamda.ridge <- ridge$lambda.min
predicteddeathRate.ridge <- predict(ridge.fit, s = optimallamda, newx = model.matrix(Model3))

predicted.ridge <- cv.glmnet(model.matrix(Model3), cancer5$deathRate, type.measure = 'mse', keep = TRUE, alpha = 0, nrow = 3041)
lamda.ridge <- predicted.ridge$lambda.min
mse2.ridge <- predicted.ridge$cvm[predicted.ridge$lambda == lamda.ridge]
mse2.ridge

rsquared1.ridge <- sum((cancer5$deathRate - mean(cancer5$deathRate))^2)
rsquared2.ridge <- sum((predicteddeathRate.ridge - cancer5$deathRate)^2)

rsquared.ridge <- (1 - (rsquared2/rsquared1))
rsquared.ridge
```

# Authors' Contributions

``` {r Author table, echo=FALSE}
crossVal=as.table(matrix(data=c('Initial analysis before variable selection and model comparison','LASSO method section','Initial analysis before variable selection','Stepwise regression section'),nrow=4,ncol=1,byrow = TRUE))
rownames(crossVal)=c('Conor','Daniel','James','Sam')
colnames(crossVal)=c('Contribution')
knitr::kable(crossVal, caption = "Author Contribution")
```
