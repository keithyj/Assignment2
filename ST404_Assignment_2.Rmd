---
title: "ST404_Assignment_2"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r, include = FALSE}
load("cancer.RData")
attach(cancer)
```
# Findings

# Statistical Methodology

Our findings are based off the preliminary EDA of the US Cancer Dataset.

## Cleaning The Data
### Missing Values
As declared in the EDA, missing values exist in PctEmployed16_Over and extremely low values exist in AvgHouseholdSize. We will clean the data in the following way:

* Scale the values between 0 and 0.1 in AvgHouseholdSize by 100. **See Appendix**
* Remove the PctEmployed16_Over column containing missing values. (Explanation of why?)

``` {r ScaleColumn, echo = FALSE}
# Multiply values less than 0.1 by 100 and replace column
AvgHouseholdSize[AvgHouseholdSize <= 0.1] <- AvgHouseholdSize[AvgHouseholdSize <= 0.1]*100
```
``` {r RemoveMissingValuesColumn, echo = FALSE}
cancer <- subset(cancer, select=-c(PctEmployed16_Over))
```

### Outliers
Several predictor variables may have outliers that can be considered for removal if they have a high leverage, specifically, incidenceRate. We will look at Cook's distance to declare if any of these outliers have significant influence.

``` {r CheckOutliers, echo = FALSE}
par(mfrow=c(1,2))
# Plot the cook's distance
plot(lm(deathRate ~ incidenceRate, data=cancer), 4)
plot(lm(deathRate ~ ., data=cancer[-c(1,4)]), 4)
```
Cook's distance, using all predictor variables, shows us that Williamsburg city in Virginia has an incidence rate with high influence and so gives us sufficient evidence to remove the outlier in incidenceRate.
``` {r removeWilliamsburg, include = FALSE}
cancer <- filter(cancer, incidenceRate != 1014.2)  
```

## Transformations

Due to the large quantity of variables, it is important to keep the model simple so that it is easy to interpret and maintains explanatory power.

Firstly, looking at skewed variables, we could perform transformations of the left-skewed variables PctPrivateCoverage and PercentMarried, and the right-skewed variables PctBlack (after a shift of 0.1 due to some values being 0), IncidenceRate, medIncome, AvgHouseholdSize, PctUnemployed16_Over and PovertyPercent. This does not totally remove skew from PctBlack but does reduce it substantially. These transformations were discussed in assignment 1 and are listed below.

Secondly, to deal with variables showing signs of heteroscedasticity, we could log transform deathRate, the dependent variable. This includes the variables medIncome, PctPrivateCoverage, PctEmpPrivCoverage and PctPublicCoverage.

Should we do some tests using diagnosis methods we can now use and decide which ones we actually wish to transform? i.e spreadlevelplot, residual plots etc.


List of potential transformed variables with associated transformation:

* PctPrivateCoverage^2^
* PercentMarried^2^
* log(PctBlack$+0.1$)
* log(IncidenceRate)
* log(medianIncome)
* log(AvgHouseholdSize)
* PctUnemplyed16_over^$\frac{1}{2}$^
* PovertyPercent^$\frac{1}{5}$
* log(deathRate)

The following residual plots will allow us to see if homoscedasticity is fixed for these variables after transformations.

``` {r transformations, echo = FALSE}
cancer2=cancer
cancer2$deathRate=log(deathRate)
cancer2$incidenceRate=boxcox(incidenceRate, p = 1.5) #using boxcox from practical class 2, user defined funciton in appendix
cancer2$medIncome=log(medIncome)
cancer2$AvgHouseholdSize=log(AvgHouseholdSize)
cancer2$PercentMarried=PercentMarried^2
cancer2$PctUnemployed16_Over=sqrt(PctUnemployed16_Over)
cancer2$PctPrivateCoverage=PctPrivateCoverage^2
cancer2$PctBlack=log(PctBlack+0.1)
```

``` {r residualplots, echo = FALSE}
par(mfrow=c(1,4))
plot(lm(deathRate ~ incidenceRate, data=cancer2), 1)
plot(lm(deathRate ~ medIncome, data=cancer2), 1)
plot(lm(deathRate ~ AvgHouseholdSize, data=cancer2), 1)
plot(lm(deathRate ~ PercentMarried, data=cancer2), 1)
plot(lm(deathRate ~ PctUnemployed16_Over, data=cancer2), 1)
plot(lm(deathRate ~ PctPrivateCoverage, data=cancer2), 1)
plot(lm(deathRate ~ PctBlack, data=cancer2), 1)
```

These potential transformations will be completed within the model itself in order for the variable names to retain their true meaning.

## Modelling and Variable Selection
### Stepwise Regression

**Initial Ideas**

Before attempting to fit any models or try stepwise regression, we looked at what variables we thought would be most important in determining the death rate within a county. We came to the conclusion that incidence rate would be the most important. Median income within a region as well as the type of health coverage in that region would be important. The median age in a county may also affect the death rate. Additionally, an intercept may need to be excluded as it makes no sense to predict the death rate to be greater than zero when all variables (particularly incidence rate) are equal to zero.

We also looked at combinations of variables that could be problematic if they are in the same model. This was based heavily on the correlation between variables, which is visualised by the matrix below: 

```{r, echo=FALSE,message=FALSE}
require(corrplot)
correlations1 <- cor(cancer[,c(2,3,5:17)],method='spearman')
corrplot(correlations1,method='square',title='Spearman Correlation plot', tl.col="black", tl.cex=0.6,mar=c(1,1,1,1))
```

The matrix shows that we should not include more than one variable explaining the type of health coverage, both of `povertyPercent` and `medIncome`, both of the median age variables and both of the marriage rate variables. 

**Without transformations**

Initially we attempted stepwise regression for the data set without any tranformations. Despite using both AIC and BIC, we had issues with there being too many parameters (10-12 parameters) and this meant that the model was not simple. As a result we chose to apply the transformations and see if that gave better results.

**With transformations**

First we apply the transformations and create a separate transformed dataset, then we define the bounds of the complexity of our model. Then we do the same as above and perform stepwise regression on this dataset using both AIC and BIC.

```{r}
cancer2=cancer
cancer2$deathRate=log(deathRate)
cancer2$incidenceRate=log(incidenceRate)
cancer2$medIncome=log(medIncome)
cancer2$AvgHouseholdSize=log(AvgHouseholdSize)
cancer2$PercentMarried=PercentMarried^2
cancer2$PctUnemployed16_Over= sqrt(PctUnemployed16_Over)
cancer2$PctPrivateCoverage= PctPrivateCoverage^2
cancer2$PctBlack = log(PctBlack+0.1)
fullTransModel=lm(deathRate~incidenceRate+medIncome+povertyPercent+MedianAgeMale+
               MedianAgeFemale+AvgHouseholdSize+PercentMarried+
               PctUnemployed16_Over+PctPrivateCoverage+PctEmpPrivCoverage+
               PctPublicCoverage+PctBlack+PctMarriedHouseholds+Edu18_24,data=cancer2)
nullTransModel=lm(deathRate~1,data=cancer2)
```

```{r}
aicTransModel=step(nullTransModel,direction='both',scope=formula(fullTransModel),trace=0,k=2)
summary(aicTransModel)
```

This model contains 13 variables which is a lot of variables and would make it hard to interpret the model. Predictive power will be good but the complexity needs to be penalised more, so we will use the BIC instead.

```{r}
bicTransModel=step(nullTransModel,direction='both',scope=formula(fullTransModel),trace=0,k=log(3047))
summary(bicTransModel)
```

This produces a model with 7 variables which is a lot more manageable and easy to interpret, it has a standard residual error that is only 0.0005 greater than the model that uses AIC. So it fits the data nearly as well, but there are far fewer variables. This model can even be further simplified because when the full output of the stepwise regression function is printed, the change in the value of the BIC is very little in the last steps compared to the first steps. We can simplify our model further by removing `PctBlack` and `PctPublicCoverage`. This reduces our R-squared value by 0.009 and our adjusted R-squared value by 0.0086 but it also removes two variables which makes the model simpler. All p-values for this model are below 0.0000000004, which is very low.

```{r}
altBICTransModel=lm(deathRate~incidenceRate+medIncome+PctPrivateCoverage+
                 PctEmpPrivCoverage+Edu18_24,data=cancer2)
summary(altBICTransModel)
```

**Multicollinearity**

```{r}
require(olsrr)
summary(altBICTransModel)
ols_coll_diag(altBICTransModel)

#Swap medIncome with povertyPercent
altBICTransModel=lm(deathRate~incidenceRate+povertyPercent+PctPrivateCoverage+
                      PctEmpPrivCoverage+Edu18_24,data=cancer2)
summary(altBICTransModel)
ols_coll_diag(altBICTransModel)
extractAIC(altBICTransModel,k=log(3047))


#Remove intercept term
altBICTransModel=lm(deathRate~0+incidenceRate+povertyPercent+PctPrivateCoverage+
                      PctEmpPrivCoverage+Edu18_24,data=cancer2)
summary(altBICTransModel)
ols_coll_diag(altBICTransModel)
extractAIC(altBICTransModel,k=log(3047))


#Remove PctEmpPrivCoverage
altBICTransModel=lm(deathRate~0+incidenceRate+povertyPercent+PctPrivateCoverage+
                      Edu18_24,data=cancer2)
summary(altBICTransModel)
ols_coll_diag(altBICTransModel)
extractAIC(altBICTransModel,k=log(3047))

#Remove Edu18_24
altBICTransModel=lm(deathRate~0+incidenceRate+povertyPercent+PctPrivateCoverage,
                    data=cancer2)
summary(altBICTransModel)
ols_coll_diag(altBICTransModel)
extractAIC(altBICTransModel,k=log(3047))
```

There are issues here in the condition numbers and the variables that contribute to them. medIncome and incidenceRate variables appear to be the issue. These are not correlated yet they seem to explain the same variance in the deathRate but both are very important in prediction. As a result it is likely best that both remain in the model due to the fact removing one would have a massive impact on the prediction power of our model. 


# Ridge Regression
Performing a Ridge regression will not contribute to any variable selection but will instead improve the predictive power of the model by shrinking parameter estimates towards 0. 
It should be used in a scenario when the # of parameters is large or when multicollinearity/collinearity exists between variables.
It will increase the bias that exists in the model, which in turn can reduce mean square error by improving variance. Despite this bias the Ridge Regression estimates will have a lower mean square error than least squared estimator.
It will be better to use another penalized likelihood method such as LASSO. However, could be potential to use it after fitting some models to improve predictive power.

# LASSO

Summary: 
Running a LASSO on the transformed data and using the min.lambda from cross validation gives less variables to include (in general it's standard to use the value of lambda which is 1se away). The variables were incidecne rate, median income, percent unemplyed and percent private cvoverage. I like this as well since these cover pretty much all the varibels we might intuitvely expect without using more than one variables measuring eseentially the same thing (ie: percent private and percent publc coverage).
Then extendning this we see an inclusion of a quadratic term for incidence rates. Interaction terms seemed to get messy so I decided not to include any. For this model I used the 1se lambda value. 
This final model appears to have some issues with normality of residuals and maybe slight violations of homoscedascity. I need to explore this further.


PLEASE IGNORE THIS 
I found a mistake in my stuff, I'm working on it atm
```{r}
require(glmnet) 
#In this code I'm using cancer 2 as defined above with the changes that incidence rate is power transformed with a p value of 1.5. I'm going to change these above #but if I haven't when someone is readign this then bear it in mind. ALso included is the region, county and state variables Connor added.

IncidenceSqrd <- cancer2[,3]^2
cancerLASSO <- cancer2[,c(3,4,12,13)]
cancerLASSO <- cbind(cancerLASSO, IncidenceSqrd)

LASSOfit <- glmnet(cancerLASSO, cancer2$deathRate, alpha = 1) #can be used for a plot 
cvLASSO <- cv.glmnet(cancerLASSO, cancer2$deathRate, alpha = 1)

LASSOcandidate <- glmnet(cancerLASSO, cancer2$deathRate, alpha = 1, lambda = cvLASSO$lambda.1se)

LASSOpredicted <- predict(LASSOcandidate, newx = cancerLASSO)
LASSOresiduals <- cancer2$deathRate - LASSOpredicted
#Note here that Union County is making plots look bad

```




















# Appendix

Loading packages
```{r}
if(!require(tidyr)){
  install.packages("tidyr")
  library(tidyr)
}
if(!require(dpylr)){
  install.packages("dpylr")
  library(dplyr)
}
```

Loading the data
```{r}
load("cancer.RData")
attach(cancer)
```
```{r histogramAvgHouseholdSize included, echo = FALSE, eval = TRUE, message = FALSE, warning = FALSE, fig.asp = 0.3, fig.cap = "Histograms of AvgHouseholdSize after scaling"}
par(mfrow = c(1,4))
hist(cancer2$AvgHouseholdSize, breaks = 30, main = "AvgHouseholdSize", ylab = "Frequency", xlab = "AvgHouseholdSize")
```

Cleaning the data
```{r}
AvgHouseholdSize[AvgHouseholdSize <= 0.1] <- AvgHouseholdSize[AvgHouseholdSize <= 0.1]*100
cancer <- subset(cancer, select=-c(PctEmployed16_Over)) 
```

Creating a Region variable, as a factor
```{r}
cancer2 = separate(cancer,"Geography",c("County","State"),sep = ", ")
cancer2 <- cancer2 %>%
  mutate(Region = case_when(
    State %in% c("New Hampshire","New Jersey","New York","Maine","Massachusetts","Vermont","Connecticut","Pennsylvania","Rhode Island") ~ "Northeast",
    State %in% c("Wisconsin","Nebraska","Michigan","Minnesota","North Dakota","Missouri","Kansas","Ohio","Indiana","Iowa","Illinois","South Dakota") ~ "Midwest",
    State %in% c("West Virginia","Virginia","North Carolina","Alabama","Arkansas","Tennessee","Texas","Louisiana","Maryland","Mississippi","Kentucky","Delaware","District of Columbia","Florida","Oklahoma","South Carolina","Georgia") ~ "South",
    State %in% c("Washington","Nevada","New Mexico","California","Montana","Utah","Colorado","Wyoming","Oregon","Hawaii","Idaho","Alaska","Arizona") ~ "West"
  ))
cancer2$Region <-  factor(cancer2$Region)
```

Power transformation UDF
```{r}
boxcox <- function(x, p){ 
  if (p == 0){
    return(log(x)) 
  }
  else{
    return((x^p - 1)/p)
  } 
}
```





