---
title: "ST404_Assignment_2"
output: 
  pdf_document:
    toc: true
    toc_depth: 2
    number_sections: true
fontsize: 11pt
linestretch: 1.5
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	fig.align = "center",
	fig.width = 7,
	message = FALSE,
	include = TRUE,
	warnings = FALSE
)
```

```{r, include = FALSE}
#Loading the data
load('cancer.rdata')
attach(cancer)
#Storing raw data
cancerRaw <- cancer
#Loading packages
library(tidyr)
library(dplyr)
library(olsrr)
library(car)
library(glmnet)
library(caret)
library(corrplot)
```

\pagebreak 

# Findings

All statements made in our findings are supported in our statistical methodology (Section 2) or from previous EDA on the data.

## Data Cleaning

Based on our previous EDA showing 152 missing observations in PctEmployed16_Over, and a high correlation between this variable and PctUnemployed16_Over, we decided to remove the PctEmployed16_Over column and ignore this for our analysis. This decision was favoured over removing the missing observation rows from the dataset as even though the dataset is large, it still contributes to more accurate analysis of the other variables. We also decided to scale the infeasibly small observations of AvgHouseholdSize (those less than 0.1) by 100. In order to account for different death rates found in different states within the US, we divided the US into 4 regions and allocated each county into one of these 4 regions. This variable, named 'Region' will then be used to construct a model. We noticed that medIncome and binnedInc are similar metrics and so we removed binnedInc from the data before we conducted any analysis. This is beacuase binnedInc is given as an interval for each county and so is not feasible to be directly used in the model.   

## Multicollinearity

We notice multicollinearity between medIncome and percentPoverty, marriage and types of insurance cover; median age variables with AvgHouseholdSize; employment statistics with type of insurance cover; marriage statistics with PctBlack. 

Furthermore we noticed that medIncome and binnedInc represent the same force and so we removed binnedInc from the data before we conducted any analysis.  

## Transformations

Some transformations of the variables were completed in order to fulfil the model assumptions, mainly homoscesastiscity (constant variance of errors) and linearity. Transformations were kept to a minimum in order to keep the model as simple as possible. One of our transformations required us to first scale PctBlack by 0.1. Hence, the transformations we use for building a model are: power transform incidenceRate and PctUnemployed16_Over as follows incidenceRate^1.5 and sqrt(PctUnemployed16_Over), also log transform PctBlack and medIncome.

## Outliers and Influencial Observations

Several observations were identified as having a high leverage and after testing for influencial observations we declared that the following observations would be removed from the data to ensure the model was not impacted by outliers.

\tiny
```{r Outlier Table, echo=FALSE}
names=c('Williamsburg City','Union County','Aleutians West Census Area','De Baca County','North Slope Borough','Chattahoochee County')
tableOutliers=c('282','1490','2714','166','2727','2682')
reasons=c('Incidence rate, Median Age Female, Percent Married, Percent Married Households, Percent Employer Private Coverage','Incidence rate','Median Age Male, Average Household Size','Average Household Size','Percent Married Households','Percent Private Coverage, Percent Employer Private Coverage')
outReasons=as.table(t(matrix(data=c(tableOutliers,reasons),ncol=6,nrow=2,byrow=TRUE)))
rownames(outReasons)=names
colnames(outReasons)=c('Index','Problematic Variable(s)')
knitr::kable(outReasons, caption = "Outlier analysis")
```

## Modelling
\normalsize
We looked for a model that had the following characteristics: simplicity, explanatory power and predictive power. The variable selection methods we used were stepwise variable selection, LASSO regression and ridge regression.

### Stepwise selection

There were issues with collinearity using this method, meaning lots of varibiables were removed. The model suggested by stepwise regression is shown below.

\tiny
```{r Model Table, echo=FALSE}
library(sylly)
name=c('incidenceRate','povertyPercent','PctUnemploy16_Over',
       'PctPublicCoverage','PctBlack','PercentMarried')
transformation=c('incidenceRate^(1.5)','None','PctUnemploy16_Over^(0.5)','None','log(PctBlack+0.1)',
                'None')
beta=c('0.00768','1.53','4.71','0.556','2.02','0.858')
interpretation=c("If incidence rate increases by a factor of x, death rate increases by a factor of x^1.5, assuming all other variables are zero", "If poverty percent increases by 1, death rate increases by 1.53, assuming all other variables remain constant", "If percent unemployed increases by a factor of x, death rate increases by a factor of x^0.5, assuming all other variables are zero", "If percent public coverage increases by 1, death rate increases by 0.556, assuming all other variables remain constant", "This variable is a little harder to interpret. If (percent black + 1) increases by a factor of x, death rate will increase by 2.02log(x), assuming all other variables remain constant", "If percent married increases by 1, death rate increases by 0.858, assuming all other variables remain constant")
modelTable=as.table(matrix(data=c(transformation,beta,interpretation),nrow=6,ncol=3))
rownames(modelTable)=name
colnames(modelTable)=c('Transformation','Parameter estimate','Interpretation')
  pander::pander(modelTable, caption = "Information on Stepwise Model", split.cell = 75, split.table = Inf, style = "grid", justify = c("left", "right", "right", "center"), use.hyphening = TRUE)
```

### LASSO regression
\normalsize
This code should be sufficent to recreate the model I made on it's own

```{r, eval = FALSE, include=FALSE}

cancer5 <- cancerRaw
cancer5$AvgHouseholdSize[cancer5$AvgHouseholdSize <= 0.1] <- cancer5$AvgHouseholdSize[cancer5$AvgHouseholdSize <= 0.1]*100
cancer5 <- cancer5[-c(2682, 282, 166, 2727, 2714, 1490),]

cancer5 = separate(cancer5,"Geography",c("County","State"),sep = ", ")
cancer5 <- cancer5 %>%
  mutate(Region = case_when(
    State %in% c("New Hampshire","New Jersey","New York","Maine","Massachusetts","Vermont","Connecticut","Pennsylvania","Rhode Island") ~ "Northeast",
    State %in% c("Wisconsin","Nebraska","Michigan","Minnesota","North Dakota","Missouri","Kansas","Ohio","Indiana","Iowa","Illinois","South Dakota") ~ "Midwest",
    State %in% c("West Virginia","Virginia","North Carolina","Alabama","Arkansas","Tennessee","Texas","Louisiana","Maryland","Mississippi","Kentucky","Delaware","District of Columbia","Florida","Oklahoma","South Carolina","Georgia") ~ "South",
    State %in% c("Washington","Nevada","New Mexico","California","Montana","Utah","Colorado","Wyoming","Oregon","Hawaii","Idaho","Alaska","Arizona") ~ "West"
  ))
cancer5$Region <-  factor(cancer5$Region)

cancerLASSO <- cancer5[,-c(1,2,5,11,19)]

cancerLASSO$incidenceRate <- boxcox(cancerLASSO$incidenceRate, p = 1.5)
cancerLASSO$PctBlack <- log(cancerLASSO$PctBlack + 0.1 )
cancerLASSO$PctUnemployed16_Over <- boxcox(cancerLASSO$PctUnemployed16_Over, p = 0.5)
cancerLASSO$medIncome <- log(cancerLASSO$medIncome)


cancerLASSO <- model.matrix(~., data = cancerLASSO)

LASSO <- glmnet(cancerLASSO, cancer5$deathRate, alpha = 1) 
cvLASSO <- cv.glmnet(cancerLASSO, cancer5$deathRate, alpha = 1)
LASSOmodel <- glmnet(cancerLASSO, cancer5$deathRate, alpha = 1, lambda = cvLASSO$lambda.1se)


LASSOpredicted <- predict(LASSOmodel, newx = cancerLASSO)
LASSOresiduals <- cancer5$deathRate - LASSOpredicted

LASSO_Rsquared <- 1 - (sum(LASSOresiduals^2)/sum((cancer5$deathRate - mean(cancer5$deathRate))^2))

```

### Ridge Regression

The ridge regression method keeps all variables in the final model and produces the following model:

**Put RIDGE model here**

## Final Model

Our final model is produced by the LASSO variable selection methodcan be summarised in the table below. 

```{r,eval=FALSE,echo=FALSE}
DF <- data.frame(row.names=c(),coefficient=c(),p-value=c())
DF
```


### Analysis of the model

Predictive power - 

Explanatory power - 

Simplicity - 

Model plots - 

```{r,eval=FALSE,echo=FALSE}
par(mfrow=c(2,2))
plot(model)
```
### Limitations


 ••• STILL NEED TO DO •••
 
### What are the major determinants of high mortality rates?

 ••• STILL NEED TO DO •••
 
### Areas of the US with unusually low or high mortality rates that do not conform to the general pattern
 
 ••• STILL NEED TO DO •••
 
We look for outliers within our model, as well as seeing if the counties we removed for having high leverage initially conform to our model and therefore to the general pattern. 



# Statistical Methodology

Our findings are based off the preliminary EDA of the US Cancer Dataset.

## Tidying the Data

As declared in the EDA, missing values exist in PctEmployed16_Over and extremely low values exist in AvgHouseholdSize. Many, 152, PctEmplyed16_Over observations are missing. We could fill in the missing values by predicting them using a linear model created by the other variables. However, the variable PctUnemployed16_Over is strongly negatively correlated with PctEmployed16_Over. Therefore, we choose to remove PctEmployed16_Over entirely to avoid collinearity with PctUnemployed16_Over, and to avoid increasing the variance of the model by performing further predictions of the missing values. We will also scale the values of AvgHouseholdSize between 0 and 0.1 by 100.

``` {r ScaleColumn, include = FALSE}
# Multiply values less than 0.1 by 100 and update column
cancer$AvgHouseholdSize[cancer$AvgHouseholdSize <= 0.1] <- cancer$AvgHouseholdSize[cancer$AvgHouseholdSize <= 0.1]*100
```

``` {r RemoveMissingValuesColumn, include = FALSE}
#removing PctEmployed16_Over and binnedInc from the dataset used for modelling.
cancer <- subset(cancer, select=-c(PctEmployed16_Over, binnedInc))
```
We add a variable describing the region in which observation the county is in. We split the US into 4 regions defined by the Census Bureau-designated regions and divisions (United States Census Bureau, 2010). These regions are Northeast, Midwest, South and West.

``` {r region variable, include=FALSE}
#Split Geography into county and state
cancer = separate(cancer,"Geography",c("County","State"),sep = ", ")
cancer <- cancer %>%
  mutate(Region = case_when(
  #Declaring the region that the states are in
    State %in% c("New Hampshire","New Jersey","New York","Maine","Massachusetts","Vermont","Connecticut","Pennsylvania","Rhode Island") ~ "Northeast",
    State %in% c("Wisconsin","Nebraska","Michigan","Minnesota","North Dakota","Missouri","Kansas","Ohio","Indiana","Iowa","Illinois","South Dakota") ~ "Midwest",
    State %in% c("West Virginia","Virginia","North Carolina","Alabama","Arkansas","Tennessee","Texas","Louisiana","Maryland","Mississippi","Kentucky","Delaware","District of Columbia","Florida","Oklahoma","South Carolina","Georgia") ~ "South",
    State %in% c("Washington","Nevada","New Mexico","California","Montana","Utah","Colorado","Wyoming","Oregon","Hawaii","Idaho","Alaska","Arizona") ~ "West"
  ))
  #Setting the Region variable as a factor
cancer$Region <-  factor(cancer$Region)
```
Lastly, we choose to ignore binnedInc in our modelling process as it represents the same force as medIncome.

## Modelling Flow
We observe the scatter plots of the individual variables against deathRate and fit an initial model based on these, as seen in previous EDA. 

``` {r redundancy, include = FALSE}
# Look at scatter plots and identify any relationships between deathRate.
par(mfrow = c(2,3))
with(cancer, scatter.smooth(deathRate~Edu18_24, ylab = "deathRate", xlab = "Edu18_24", lpars = list(col = "red", lwd = 2), main = "deathRate against Edu18_24", cex.main = 1))
with(cancer, scatter.smooth(deathRate~PctMarriedHouseholds, ylab = "deathRate", xlab = "PctMarriedHouseholds", lpars = list(col = "red", lwd = 2), main = "deathRate against PctMarriedHouseholds", cex.main = 1))
with(cancer, scatter.smooth(deathRate~PctBlack, ylab = "deathRate", xlab = "PctBlack", lpars = list(col = "red", lwd = 2), main = "deathRate against PctBlack", cex.main = 1))
with(cancer, scatter.smooth(deathRate~PctPrivateCoverage, ylab = "deathRate", xlab = "PctPrivateCoverage", lpars = list(col = "red", lwd = 2), main = "deathRate against PctPrivateCoverage", cex.main = 1))
with(cancer, scatter.smooth(deathRate~PctPublicCoverage, ylab = "deathRate", xlab = "PctPublicCoverage", lpars = list(col = "red", lwd = 2), main = "deathRate against PctPublicCoverage", cex.main = 1))
with(cancer, scatter.smooth(deathRate~povertyPercent, ylab = "deathRate", xlab = "povertyPercent", lpars = list(col = "red", lwd = 2), main = "deathRate against povertyPercent", cex.main = 1))
with(cancer, scatter.smooth(deathRate~medIncome, ylab = "deathRate", xlab = "medIncome", lpars = list(col = "red", lwd = 2), main = "deathRate against medIncome", cex.main = 1))
with(cancer, scatter.smooth(deathRate~incidenceRate, ylab = "deathRate", xlab = "incidenceRate", lpars = list(col = "red", lwd = 2), main = "deathRate against incidenceRate", cex.main = 1))
with(cancer, scatter.smooth(deathRate~Region, ylab = "deathRate", xlab = "Region", lpars = list(col = "red", lwd = 2), main = "deathRate against Region", cex.main = 1))
with(cancer, scatter.smooth(deathRate~MedianAgeMale, ylab = "deathRate", xlab = "MedianAgeMale", lpars = list(col = "red", lwd = 2), main = "deathRate against MedianAgeMale", cex.main = 1))
with(cancer, scatter.smooth(deathRate~MedianAgeFemale, ylab = "deathRate", xlab = "MedianAgeFemale", lpars = list(col = "red", lwd = 2), main = "deathRate against MedianAgeFemale", cex.main = 1))
with(cancer, scatter.smooth(deathRate~AvgHouseholdSize, ylab = "deathRate", xlab = "AvgHouseholdSize", lpars = list(col = "red", lwd = 2), main = "deathRate against AvgHouseholdSize", cex.main = 1))
with(cancer, scatter.smooth(deathRate~PercentMarried, ylab = "deathRate", xlab = "PercentMarried", lpars = list(col = "red", lwd = 2), main = "deathRate against PercentMarried", cex.main = 1))
with(cancer, scatter.smooth(deathRate~PctEmpPrivCoverage, ylab = "deathRate", xlab = "PctEmpPrivCoverage", lpars = list(col = "red", lwd = 2), main = "deathRate against PctEmpPrivCoverage", cex.main = 1))
with(cancer, scatter.smooth(deathRate~PctUnemployed16_Over, ylab = "deathRate", xlab = "PctUnemployed16_Over", lpars = list(col = "red", lwd = 2), main = "deathRate against PctUnemployed16_Over", cex.main = 1))
```
The variables we will include in the initial model are those that show a relationship with deathRate. From the scatter plots it is not clear to see whether this is the case, so instead we will fit an initial model of the current data set and check the significance.

``` {r adequacy, include = FALSE}
# Creating a linear model with all variables from the current dataset
Model0 <- lm(deathRate ~ Region + incidenceRate + medIncome + povertyPercent + MedianAgeMale + MedianAgeFemale + AvgHouseholdSize + PercentMarried + PctBlack + PctMarriedHouseholds + PctPrivateCoverage + PctPublicCoverage + PctEmpPrivCoverage + PctUnemployed16_Over + Edu18_24, data = cancer)

# Check the output of the model showing estimates and significance
summary(Model0)
```

We test the null hypothesis that there is no linear association between deathRate and povertyPercent, deathRate and medianAgeMale, and deathRate and PercentMarried. We check the p values for these which show 0.656,  0.936, 0.360, and 0.141, hence we fail to reject the null hypothesis. We will not remove any of these variables from the model at this moment in time as there could be underlying issues as to why they are not significant, such as multicollinearity. 

We will assess the adequacy of the model by checking if the functional form of the model is correct. We will look at the following plots to assess this:


``` {r, echo = FALSE, eval = TRUE}
# Identify any heteroscedascity and non-linearity
par(mfrow=c(1,2))
plot(Model0, 1)
plot(Model0, 2)
```

We can see from the plots that there exists heteroscedascity and non-linearity. We also notice from our intitial EDA that some individual predictor variables are not linear and show signs of heteroscedascity, so we will need to perform some transformations of some variables. We do this by using Box-Cox and observations after trialling log transformations. We first transform the individual predictor variables to satisfy homoscedascity, these include the following transformations:

* log(PctBlack$+0.1$)
* incidenceRate^1.5
* sqrt(PctUnemployed16_Over)
* log(medIncome)

```{r, include=FALSE}
#Box cox Power transformation UDF
boxcox <- function(x, p){ 
  if (p == 0){
    return(log(x)) 
  }
  else{
    return((x^p - 1)/p)
  } 
}
```

``` {r transformations, include = FALSE}
# Declare new dataframe from old
cancer5=cancer
#Scaling PctBlack 
cancer5$PctBlack=(cancer5$PctBlack) + 0.1
# Transformations
cancer5$incidenceRate=boxcox(cancer5$incidenceRate, p = 1.5)
cancer5$medIncome=log(cancer5$medIncome)
cancer5$PctUnemployed16_Over=boxcox(cancer5$PctUnemployed16_Over, p = 0.5)
cancer5$PctBlack=log(cancer5$PctBlack)

```

``` {r checkingassumptions, echo = FALSE, eval = TRUE}
#Check for any updated heteroscedascity and linearity
par(mfrow=c(2,2))
plot(lm(deathRate ~ incidenceRate, data = cancer5), 1)
plot(lm(deathRate ~ incidenceRate, data = cancer5), 3)
plot(lm(deathRate ~ medIncome, data = cancer5), 1)
plot(lm(deathRate ~ medIncome, data = cancer5), 3)
plot(lm(deathRate ~ PctBlack, data = cancer5), 1)
plot(lm(deathRate ~ PctBlack, data = cancer5), 3)
plot(lm(deathRate ~ PctUnemployed16_Over, data = cancer5), 1)
plot(lm(deathRate ~ PctUnemployed16_Over, data = cancer5), 3)
```

These transformations show evidence of homoscedascity and linearity thus we will update the new model with the transformed variables.

``` {r adequacy2, echo = FALSE, eval = TRUE}
# Creating updated model with transformed variables.
Model1 <- lm(deathRate ~ Region + incidenceRate + medIncome + povertyPercent + MedianAgeMale + MedianAgeFemale + AvgHouseholdSize + PercentMarried + PctBlack + PctMarriedHouseholds + PctPrivateCoverage + PctPublicCoverage + PctEmpPrivCoverage + PctUnemployed16_Over + Edu18_24, data = cancer5)

# Identify any heteroscedascity and non-linearity 
par(mfrow=c(1,2))
plot(Model1, 1)
plot(Model1, 3)
```
The new model presents linearity and the heteroscedascity has been reduced, but it still exists. This could be due to influencial outliers.

#### Outliers  


Outliers are not always a cause for concern: however, if they have high influence then they will need to be considered for removal. Influencial outliers are those that effect the slope of the model. We will go on to calculate if any outliers exist and if they have high leverage, then deduce if these are influencial by using residual leverage plots and DFBETAS.


``` {r CheckOutliers, include = FALSE}
par(mfrow=c(1,2))
ols_plot_resid_lev(Model1)
# This plot shows that the observations that are outliers with high leverage that we declare highly influencial are 2682, 282, 166, 2727, and 2714.
par(mfrow=c(2,2))
ols_plot_dfbetas(Model1)
# This plot shows that the observations 2682, 282, 166, 2727, and 2714 all have a DFBETA > 0.4.
```

Observations 2682, 282, 166, 2727, 1490 and 2714 appear to be problematic appearing as high leverage, high outliers and also being influencial observations as shown by the DFBETAS > 0.4 we will declare this to be highly influencial and so remove these observations from our model. We investigated these further for the nature of the influence, our research reinforced our decision to remove as for example Union County, Florida is a county where prisoners with lung cancer are sent. **Need reference**.

``` {r removeoutliers, include = FALSE}
#remove the influencial outliers.
library(dplyr)
cancer5 <-  slice(cancer5, -c(2682, 282, 166, 2727, 2714, 1490))
```

``` {r Model3, include = FALSE}
Model3 <- lm(deathRate ~ Region + incidenceRate + medIncome + povertyPercent + MedianAgeMale + MedianAgeFemale + AvgHouseholdSize + PercentMarried + PctBlack + PctMarriedHouseholds + PctPrivateCoverage + PctPublicCoverage + PctEmpPrivCoverage + PctUnemployed16_Over + Edu18_24, data = cancer5)
par(mfrow=c(2,2))
plot(Model3)
```

``` {r checklargesample, include = FALSE}
summary(cancer5)
#Shows 3041 observations 
```
We have 3041 observations in our sample space, this is large and so we can pay less attention to the errors being normally distributed as it is not a major concern. The new model shows linearity and improved homoscedascitity and so we check if deathRate highly correlates to any of the explanatory variables by analysing the model's summary statistics and checking the p values for significance. The F statistic is high with a p-value <0.05. So, we can determine that our response variable has a relationship with at least one of the variables in the model and we can further test which model is the optimal model by testing for redundancy. We will spot this by comparing the summary statistics output with the scatter plots we saw in our EDA. 

The scatter plots show a negative correlation between deathRate and PctMarriedHouseholds and PctEmpPrivCoverage, which are not negative in the summary of the model. Moreover, the estimate is negative for PovertyPercent and PctBlack in the summary statistics but clearly not in the plot. This shows there exists redundancy and can be explained potentially by any collinearity. The other redundancy we see is that MedianAgeMale and MedianAgeFemale plot against deathRate does not correlate with the estimate in the summary statistic. These redundancies could be caused by multicollinearity.

``` {r summarymod3, include = FALSE}
summary(Model3)
```

``` {r multicolliearity2, include = FALSE}
# Look at scatter plots and compare weak positive correlations to the correlations shown in original plots.
par(mfrow = c(2,3))
with(cancer5, scatter.smooth(deathRate~Edu18_24, ylab = "deathRate", xlab = "Edu18_24", lpars = list(col = "red", lwd = 2), main = "deathRate against Edu18_24", cex.main = 1))
with(cancer5, scatter.smooth(deathRate~PctMarriedHouseholds, ylab = "deathRate", xlab = "PctMarriedHouseholds", lpars = list(col = "red", lwd = 2), main = "deathRate against PctMarriedHouseholds", cex.main = 1))
with(cancer5, scatter.smooth(deathRate~PctBlack, ylab = "deathRate", xlab = "PctBlack", lpars = list(col = "red", lwd = 2), main = "deathRate against PctBlack", cex.main = 1))
with(cancer5, scatter.smooth(deathRate~PctPrivateCoverage, ylab = "deathRate", xlab = "PctPrivateCoverage", lpars = list(col = "red", lwd = 2), main = "deathRate against PctPrivateCoverage", cex.main = 1))
with(cancer5, scatter.smooth(deathRate~PctPublicCoverage, ylab = "deathRate", xlab = "PctPublicCoverage", lpars = list(col = "red", lwd = 2), main = "deathRate against PctPublicCoverage", cex.main = 1))
with(cancer5, scatter.smooth(deathRate~povertyPercent, ylab = "deathRate", xlab = "povertyPercent", lpars = list(col = "red", lwd = 2), main = "deathRate against povertyPercent", cex.main = 1))
with(cancer5, scatter.smooth(deathRate~medIncome, ylab = "deathRate", xlab = "medIncome", lpars = list(col = "red", lwd = 2), main = "deathRate against medIncome", cex.main = 1))
with(cancer5, scatter.smooth(deathRate~incidenceRate, ylab = "deathRate", xlab = "incidenceRate", lpars = list(col = "red", lwd = 2), main = "deathRate against incidenceRate", cex.main = 1))
with(cancer5, scatter.smooth(deathRate~Region, ylab = "deathRate", xlab = "Region", lpars = list(col = "red", lwd = 2), main = "deathRate against Region", cex.main = 1))
with(cancer5, scatter.smooth(deathRate~MedianAgeMale, ylab = "deathRate", xlab = "MedianAgeMale", lpars = list(col = "red", lwd = 2), main = "deathRate against MedianAgeMale", cex.main = 1))
with(cancer5, scatter.smooth(deathRate~MedianAgeFemale, ylab = "deathRate", xlab = "MedianAgeFemale", lpars = list(col = "red", lwd = 2), main = "deathRate against MedianAgeFemale", cex.main = 1))
with(cancer5, scatter.smooth(deathRate~AvgHouseholdSize, ylab = "deathRate", xlab = "AvgHouseholdSize", lpars = list(col = "red", lwd = 2), main = "deathRate against AvgHouseholdSize", cex.main = 1))
with(cancer5, scatter.smooth(deathRate~PercentMarried, ylab = "deathRate", xlab = "PercentMarried", lpars = list(col = "red", lwd = 2), main = "deathRate against PercentMarried", cex.main = 1))
with(cancer5, scatter.smooth(deathRate~PctEmpPrivCoverage, ylab = "deathRate", xlab = "PctEmpPrivCoverage", lpars = list(col = "red", lwd = 2), main = "deathRate against PctEmpPrivCoverage", cex.main = 1))
with(cancer5, scatter.smooth(deathRate~PctUnemployed16_Over, ylab = "deathRate", xlab = "PctUnemployed16_Over", lpars = list(col = "red", lwd = 2), main = "deathRate against PctUnemployed16_Over", cex.main = 1))
```

#### Multicollinearity
``` {r multicollinearity, include = FALSE}
library(olsrr)
ols_coll_diag(Model3)
#Look for low tolerance, VIF > 5 and condition index > 30
```
This shows that PctMarriedHouseholds, PctPrivateCoverage, PercentMarried, povertyPercent and medIncome all had VIF >5. When looking at the condition index's above 30, the variance decomposition proportion above 0.3, which we used as the cut off echoing Liao et al (2012), is PctEmpPrivCoverage and Edu18_24 when condition index is 45.92. When the condition index is 50.24, PctPublicCoverage is an issue. When the condition index is 80.46, PctPrivateCoverage and Edu19_24 are issues. When the condition index is 101.87, issues lie with PercentMarried and PctMarriedHouseholds. When condition index is 124.7, issues lie with MedianAgeMale & Female. When condition index is 174.74, the issues are with AvgHouseholdSize, PercentMarried and PctMarriedHouseholds. When condition index is 679.76: the intercept, medIncome, and povertyPercent presents an issue.

We can see that the variables that show, to some degree, multicollinearity are (in order of significance): povertyPercent and medIncome; AvgHouseholdSize, PercentMarried and PctMarriedHouseholds; MedianAgeMale and MedianAgeFemale; PercentMarried and PctMarried Households, PctPrivateCoverage and Edu18_24; PctPublicCoverage and Edu18_24; and lastly PctEmpPrivCoverage and Edu18_24.

```{r, include=FALSE}
#defining boxcox function
boxcox <- function(x, p){ 
  if (p == 0){
    return(log(x)) 
  }
  else{
    return((x^p - 1)/p)
  } 
}
```

### Stepwise Regression

**Initial Ideas**

Before attempting to fit any models or try stepwise regression, we looked at what variables we thought would be most important in determining the death rate within a county. In summary, incidence rate is the most important followed by the type of insurance and an indicator of wealth. Also an intercept term does not make much sense as if there is no incidence rate then there should be no deaths. 

**AIC Stepwise Regression**

We first attempted to use AIC but this returned a model with 9 variables in it as well as an intercept term. Despite the fact this model is a good explanation of the data, it is far from simple and would be more difficult to interpret properly. As a result we opted to go forward using BIC as this gave a heavier penalty for having more variables.

```{r Define null and full models, include=FALSE}
fullTransModel=lm(deathRate ~ Edu18_24 + medIncome + PctMarriedHouseholds + 
                    PctBlack + PctPublicCoverage + PctPrivateCoverage + 
                    PctUnemployed16_Over + PercentMarried + povertyPercent + 
                    incidenceRate + Region,
                  data = cancer5)
nullTransModel=lm(deathRate~1,data=cancer5)
```


```{r AIC Stepwise, include=FALSE}
AICmodel1 = step(nullTransModel,direction='both',scope=formula(fullTransModel))
ols_coll_diag(AICmodel1)
```

**BIC Stepwise Regression**

```{r BIC Stepwise 1, include=FALSE}
BICmodel1 = step(nullTransModel,direction='both',scope=formula(fullTransModel), k=log(3040))
ols_coll_diag(BICmodel1)
summary(BICmodel1)
```

Our first model for the BIC stepwise regression model had 6 variables in it which is a much easier to interpret than what the AIC suggested. However, the condition number was 265. This means that two of the rows in the condition matrix are near to liner combinations of each other and so we have an issue with collinearity. The output suggested that it was the median income and the intercept term. We initially thought that having an intercept term may not be the best approach going forward and so we removed it at this point.

```{r BIC Stepwise 2, include=FALSE}
fullTransModel=lm(deathRate ~ 0 + Edu18_24 + medIncome + PctMarriedHouseholds + 
                    PctBlack + PctPublicCoverage + PctPrivateCoverage + 
                    PctUnemployed16_Over + PercentMarried + povertyPercent + 
                    incidenceRate + Region,
                  data = cancer5)
nullTransModel=lm(deathRate~0,data=cancer5)

BICmodel2 = step(nullTransModel,direction='both',scope=formula(fullTransModel), k=log(3040))
ols_coll_diag(BICmodel2)
summary(BICmodel2)
```

Despite removing the intercept term, there were still collinearity issues between the median income, education coefficient and regions. Median income appeared to be a common theme and there were alternative measures of wealth available. As a result we removed median income from our full model and looked at if the collinearity was solved then.

```{r BIC Stepwise 3, include=FALSE}
fullTransModel=lm(deathRate ~ 0 + Edu18_24 + PctMarriedHouseholds + 
                    PctBlack + PctPublicCoverage + PctPrivateCoverage + 
                    PctUnemployed16_Over + PercentMarried + povertyPercent + 
                    incidenceRate + Region,
                  data = cancer5)

BICmodel3 = step(nullTransModel,direction='both',scope=formula(fullTransModel), k=log(3040))
ols_coll_diag(BICmodel3)
summary(BICmodel3)
```

The condition number decreased but was still 65, which is too large. At this point the problematic variables were the regions, percentage of married households and the education coefficient. We decided that we would remove the regions since if a region has a higher death rate then this is usually captured by a higher incidence rate. As a result, we removed the regions from our full model and repeated the stepwise regression again.

```{r BIC Stepwise 4, include=FALSE}
fullTransModel=lm(deathRate ~ 0 + Edu18_24 + PctMarriedHouseholds + 
                    PctBlack + PctPublicCoverage + PctPrivateCoverage + 
                    PctUnemployed16_Over + PercentMarried + povertyPercent + 
                    incidenceRate,
                  data = cancer5)

BICmodel4 = step(nullTransModel,direction='both',scope=formula(fullTransModel), k=log(3040))
ols_coll_diag(BICmodel4)
summary(BICmodel4)
```

After analysing the condition numbers and the VIFs from this model we were able to conclude that collinearity was no longer an issue. The condition number was below 19 and the VIFs were all below 3.5. Consequently, we decided that we could use this model as the model selected by stepwise regression.

**Check Model Assumptions**

The model produces the following plots:

```{r Check Assumptions, echo=FALSE}
par(mfrow=c(2,2))
for (i in (1:4)) {
  plot(BICmodel4,i)
}
```

From these graphs we can see that the Residuals vs Fitted plot and the Scale-Location plot look very slightly quadratic which is shown by the red line. However, the resdiuals appear to be randomly distributed around 0 and there seems to be no correlation between them. There also does not appear to be issues with heteroscedsticity. The errors in the Normal Q-Q plot appear to fall away from the line in the tails but we have 3040 data points and so the normality in errors does not need to be followed as strictly. Finally the Cook's Distance plot shows there are some points which had some influence but there was no justification for omitting them as an outlier.

**Cross validation**

To determine how well the model predicts values we use leave one out cross validation (LOOCV). This is included in the package `caret`. We ran this on the model suggested by BIC stepwise regression and the summary is given in the table below.

```{r Cross validation, echo=FALSE}
crossVal=as.table(matrix(data=c(20.98,0.4176,16.09),nrow=1,ncol=3,byrow = TRUE))
rownames(crossVal)=c('BIC stepwise model')
colnames(crossVal)=c('Root Mean Squared Error','Mean Absolute Error','R-Squared')
knitr::kable(crossVal, caption = "Errors and goodness of fit measures for BIC stepwise regression model")
```

## LASSO

Note: might want to say ran an LAR algorithim here, I think that might be what it is called.

Running a LASSO on our data with transformation as outlined above. By using cross validation we get the value of lambda minimising the mean squared error as (fill). However as illustrated in figure ? we can see that this value of lambda includes all coeficents in our model. This would make our model difficult to interpret so we instead consider the value of lambda which is 1se away (fill). Using this more strict restriction term shrinks the following variables to zero: (fill). 

By using LASSO variable selection we construct a model with eight predictor variables which allign with those we expected from our intial EDA. The variables we have are

### Put the coefficents plot here with abline used for the values of lambda also include a legend ###


---Extending the LASSO---
In attempting to extend our existing LASSO model we first included interaction terms for variables with high correlation. This included (fill). However by including these variables we ran into more issues with the variables being selected. Therefore we chose to omit the inclusion of interaction terms. With respect to higher order terms we found that the only quadratic terms being included would shrink their linear ocunterpart to zero. Therefore we also decided to omit higher order terms for independent variables from the model.

---Evaluating the goodness of this model--- 
The R squared value for this model was (fill).
The adjusted R Squared value was (fill)
The variance inflation factor for the variables included did not exceed 2, suggesting no issuees with multicolinearity. 
Residuals are homoscedastic and uncorrelated. However the residuals did not demonstrate a normal distribution. 

```{r, eval = FALSE}
cancerLASSO <- cancer5[,-c(1,2,17)]

cancerLASSO <- model.matrix(~., data = cancerLASSO)

LASSO <- glmnet(cancerLASSO, cancer3$deathRate, alpha = 1) 
set.seed(6)
cvLASSO <- cv.glmnet(cancerLASSO, cancer3$deathRate, alpha = 1)
LASSOmodel <- glmnet(cancerLASSO, cancer3$deathRate, alpha = 1, lambda = cvLASSO$lambda.1se)


LASSOpredicted <- predict(LASSOmodel, newx = cancerLASSO)
LASSOresiduals <- cancer3$deathRate - LASSOpredicted

LASSO_Rsquared <- 1 - (sum(LASSOresiduals^2)/sum((cancer3$deathRate - mean(cancer3$deathRate))^2))
```

```{r Model Table, echo = FALSE}
library(sylly)
name=c('incidenceRate','medIncome','PctUnemploy16_Over','PctPrivateCoverage',
       'PctPublicCoverage','Edu18_24','RegionNorthaast','RegionSouth','RegionWest')
beta=c('0.010','-23.5','2.49','-0.212','0.127','-8.22','-4.61','3.73','-12.4')
modelTable=as.table(matrix(data=c(beta),nrow=9,ncol=1))
rownames(modelTable)=name
colnames(modelTable)=c('Parameter estimate')
pander::pander(modelTable, caption = "Information on Stepwise Model", split.cell = 75, split.table = Inf, style = "grid", justify = c("left","center"), use.hyphening = TRUE)
```

```{r, echo = FALSE}
par(mfrow = c(1,1))
bin <- scale(LASSOresiduals)
bin1 <- which(bin < 0)
bin[bin1] <- -bin[bin1]
plot(LASSOpredicted, sqrt(bin), main = "Scale Location", xlab = "Fitted values", ylab = "Square Root of Standardised Residuals")
plot(LASSOpredicted, LASSOresiduals, main = "Residuals vs Fitted plot for LASSO model", xlab = "Fitted values", ylab = "Residuals")
```



## Ridge Regression
Due to multicollinearity existing in the model we will perform a ridge regression, this will not contribute to any variable selection but will instead improve the predictive power of the model by shrinking parameter estimates towards 0. 

``` {r ridge, echo = FALSE, eval = FALSE}
set.seed(3041)
ridge.fit <- cv.glmnet(model.matrix(Model3), cancer5$deathRate, alpha = 0, nrow = 3041)

optimallamda.ridge <- ridge$lambda.min
predicteddeathRate.ridge <- predict(ridge.fit, s = optimallamda, newx = model.matrix(Model3))

predicted.ridge <- cv.glmnet(model.matrix(Model3), cancer5$deathRate, type.measure = 'mse', keep = TRUE, alpha = 0, nrow = 3041)
lamda.ridge <- predicted.ridge$lambda.min
mse2.ridge <- predicted.ridge$cvm[predicted.ridge$lambda == lamda.ridge]
mse2.ridge

rsquared1.ridge <- sum((cancer5$deathRate - mean(cancer5$deathRate))^2)
rsquared2.ridge <- sum((predicteddeathRate.ridge - cancer5$deathRate)^2)

rsquared.ridge <- (1 - (rsquared2/rsquared1))
rsquared.ridge
```

The Ridge regression produces a linear model with an R^2 value of 0.484, and a mean square error of 378.5, which will be a reduced value as the ridge regression has improved variance at the cost of increased bias in the model. The model still contains all predictor variables and so would be good to improve the predictive power of the model, but considering all variables are still included, an R^2 value of 4.84 is not great, and it will be better to use another penalized likelihood method such as LASSO.

## Comparison of models found from variable selection




## Limitations and recommendations for improvements in future work

One limitation of this data is that the populations in each county are different. However, our model puts equal weighting on each county. To gain a more realistic model, perhaps it would be valuable to give different weightings to counties depending on their population.




















# Appendix

```{r}
#Correlation of employment variables
cancerNoNAs <- na.omit(cancerRaw)
cor(cancerNoNAs$PctUnemployed16_Over,cancerNoNAs$PctEmployed16_Over)
```

```{r,eval=FALSE}
#Cleaning the data
cancer$AvgHouseholdSize[cancer$AvgHouseholdSize <= 0.1] <- cancer$AvgHouseholdSize[cancer$AvgHouseholdSize <= 0.1]*100
cancer <- subset(cancer, select=-c(PctEmployed16_Over)) 
```

Showing fixed AvgHouseholdSize

```{r histogramAvgHouseholdSize included, echo = FALSE, eval = TRUE, message = FALSE, warning = FALSE, fig.asp = 0.4, fig.cap = "Histograms of AvgHouseholdSize after scaling small values"}
par(mfrow = c(1,2))
hist(cancerRaw$AvgHouseholdSize, breaks = 30, main = "Raw AvgHouseholdSize", ylab = "Frequency", xlab = "AvgHouseholdSize")
hist(cancer$AvgHouseholdSize, breaks = 30, main = "Corrected AvgHouseholdSize", ylab = "Frequency", xlab = "AvgHouseholdSize")
```


```{r,eval=FALSE}
#Creating a Region variable, as a factor
cancer2 = separate(cancer,"Geography",c("County","State"),sep = ", ")
cancer2 <- cancer2 %>%
  mutate(Region = case_when(
    State %in% c("New Hampshire","New Jersey","New York","Maine",
                 "Massachusetts","Vermont","Connecticut","Pennsylvania",
                 "Rhode Island") ~ "Northeast",
    State %in% c("Wisconsin","Nebraska","Michigan","Minnesota",
                 "North Dakota","Missouri","Kansas","Ohio",
                 "Indiana","Iowa","Illinois","South Dakota") ~ "Midwest",
    State %in% c("West Virginia","Virginia","North Carolina","Alabama",
                 "Arkansas","Tennessee","Texas","Louisiana",
                 "Maryland","Mississippi","Kentucky","Delaware",
                 "District of Columbia","Florida","Oklahoma","South Carolina",
                 "Georgia") ~ "South",
    State %in% c("Washington","Nevada","New Mexico","California",
                 "Montana","Utah","Colorado","Wyoming",
                 "Oregon","Hawaii","Idaho","Alaska","Arizona") ~ "West"
  ))
cancer2$Region <-  factor(cancer2$Region)
```

```{r, eval=FALSE}
#Power transformation UDF
boxcox <- function(x, p){ 
  if (p == 0){
    return(log(x)) 
  }
  else{
    return((x^p - 1)/p)
  } 
}
```

# References

Liao D, Valliant R. 2012. Condition indexes and variance decompositions for diagnosing collinearity in linear model analysis of survey data. *Survey Methodology*. **38**(2),pp.189–202.

United States Census Bureau. 2010. *United States Census Bureau*. [Online]. [Accessed 8th March 2022]. Available from: https://www2.census.gov/geo/pdfs/maps-data/maps/reference/us_regdiv.pdf



